{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zQjLAnDgacJl",
        "BGJn6eFvaod6",
        "IIFccVI7KVGe"
      ],
      "gpuType": "V100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/fall_2023_210_capstone_deepika_srila/blob/main/srila/code_snippet/wip/video_data_collection_love.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps**\n",
        "\n",
        "<li>Creates the directory structure</li>\n",
        "<li>Reads the input from private github</li>\n",
        "<li>Processes the input data (gif/embedded video/youtube video/youtube shorts)\n",
        "<li>Transforms the input video (grayscaling, resizing, padding) and adding random augmentation.\n",
        "<li>Applies systemetic transformation on each frame of each transformed video data and random transformation on each frame of each augmented video data.</li>\n",
        "<li>Combines all the videos and split in multilabel stratification format.</li>\n",
        "<li>Combines all the images and split in multilabel stratification format.</li>\n",
        "<li>Model building and evaluation.</li\n"
      ],
      "metadata": {
        "id": "fwscFg8o0KHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing New Libraries"
      ],
      "metadata": {
        "id": "zQjLAnDgacJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QTVSp36caXYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4f47e5-045f-4c93-99cb-08e4cd20ce73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.23.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n",
            "Collecting kora\n",
            "  Downloading kora-0.9.20-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from kora) (7.34.0)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from kora) (1.5.29)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore->kora) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->kora) (23.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->kora)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->kora) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->kora) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->kora) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kora) (0.2.12)\n",
            "Installing collected packages: jedi, kora\n",
            "Successfully installed jedi-0.19.1 kora-0.9.20\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Collecting rembg\n",
            "  Downloading rembg-2.0.52-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg) (4.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.23.5)\n",
            "Collecting onnxruntime (from rembg)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg) (4.8.1.78)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from rembg) (9.4.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg) (1.8.0)\n",
            "Collecting pymatting (from rembg)\n",
            "  Downloading PyMatting-1.1.12-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg) (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rembg) (4.66.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.13.2)\n",
            "Collecting coloredlogs (from onnxruntime->rembg)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (1.12)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (4.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (2.31.0)\n",
            "Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg) (0.58.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (1.5.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2023.11.17)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->rembg)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->rembg) (1.3.0)\n",
            "Installing collected packages: humanfriendly, pymatting, coloredlogs, onnxruntime, rembg\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.16.3 pymatting-1.1.12 rembg-2.0.52\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting PyGithub\n",
            "  Downloading PyGithub-2.1.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynacl>=1.4.0 (from PyGithub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.31.0)\n",
            "Collecting pyjwt[crypto]>=2.4.0 (from PyGithub)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.5.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.0.7)\n",
            "Collecting Deprecated (from PyGithub)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2023.11.17)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->PyGithub) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
            "Installing collected packages: pyjwt, Deprecated, pynacl, PyGithub\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "Successfully installed Deprecated-1.2.14 PyGithub-2.1.1 pyjwt-2.8.0 pynacl-1.5.0\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m931.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Collecting scikit-video\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from scikit-video) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.11.4)\n",
            "Installing collected packages: scikit-video\n",
            "Successfully installed scikit-video-1.1.11\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.23.5)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug) (4.8.0.76)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.31.6)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.0.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (3.2.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (2.8.2)\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m688.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: scikit-video in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from scikit-video) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.11.4)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=b235eaeaaf1a56fea28e15c6fa9c83e017e739278e3ffb26e7e122ac02da58d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: ffmpy\n",
            "Successfully installed ffmpy-0.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube\n",
        "!pip install moviepy\n",
        "!pip install -U kora\n",
        "!pip install opencv-python\n",
        "!pip install rembg\n",
        "!pip install -q mediapipe==0.10.0\n",
        "!pip install ffmpeg-python\n",
        "!pip install PyGithub\n",
        "!pip install scikit-multilearn\n",
        "!pip install scikit-video\n",
        "!pip install imageio\n",
        "!pip install imgaug\n",
        "!pip install einops\n",
        "!pip install tqdm\n",
        "!pip install scikit-video\n",
        "!pip install ffmpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing New Libraries"
      ],
      "metadata": {
        "id": "BGJn6eFvaod6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import tqdm\n",
        "import sys\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "import einops\n",
        "from pathlib import Path\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shutil\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import copy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import urllib.request\n",
        "from github import Github\n",
        "import subprocess\n",
        "import uuid\n",
        "import requests\n",
        "from requests.structures import CaseInsensitiveDict\n",
        "import json\n",
        "import urllib\n",
        "import copy\n",
        "import io\n",
        "import linecache\n",
        "from sklearn.utils import shuffle\n",
        "import time\n",
        "import cv2\n",
        "import ffmpeg\n",
        "from moviepy.editor import *\n",
        "import ffmpy\n",
        "\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "\n",
        "from rembg import remove\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "\n",
        "from kora.drive import upload_public\n",
        "import IPython.display as ipd\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from pytube import YouTube\n",
        "import moviepy.editor as mved\n",
        "import moviepy.video.io.ImageSequenceClip\n",
        "import mediapipe as mp\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "import imageio\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "from imutils import paths\n",
        "import skimage.feature as feature\n",
        "import matplotlib.image\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from skimage.feature import hog\n",
        "from scipy.ndimage import convolve\n",
        "from scipy import signal\n",
        "import skimage.exposure as exposure\n",
        "import skimage.filters as filters\n",
        "from skimage.filters import prewitt_h\n",
        "from skimage.filters import prewitt_v\n",
        "from PIL import Image\n",
        "from PIL import ImageSequence\n",
        "from PIL import ImageEnhance\n",
        "from PIL import ImageFilter\n",
        "from scipy import ndimage\n",
        "import moviepy.video.io.ImageSequenceClip\n",
        "import random\n",
        "import skimage\n",
        "import skvideo.io\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "os.environ['GDRIVE_CONFIG_DIR'] = \"/content/drive/MyDrive\"\n",
        "\n",
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
        "from scipy import stats\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv3D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling3D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Input\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import GlobalAveragePooling3D\n",
        "from keras.layers import Lambda\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.layers import Add\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications import ResNet50"
      ],
      "metadata": {
        "id": "VRMe-6kmarIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb391d4-6879-45ec-ea38-3ad1aa44d6aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Variables"
      ],
      "metadata": {
        "id": "IIFccVI7KVGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = [\"love\"]\n",
        "label_map = {label: num for num, label in enumerate(label_list)}"
      ],
      "metadata": {
        "id": "JmDJcBUzKan-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_file_list = pd.DataFrame()"
      ],
      "metadata": {
        "id": "2v6WajnPHLLf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_count = -1"
      ],
      "metadata": {
        "id": "SPNHzBTtTKDi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The following dictionary maps the input file to the generated file name,\n",
        "which will be used afterwards. For troubleshooting, we need to have the mapping\n",
        "to trace back the problematic input source.\n",
        "\"\"\"\n",
        "input_output_map_dict = {}\n",
        "for label in label_list:\n",
        "    input_output_map_dict[label] = []"
      ],
      "metadata": {
        "id": "Uo17ANd516eZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This dictionary will be populated later on, once all the relevent label data have\n",
        "been moved under consolidated_data folder.\n",
        "\"\"\"\n",
        "file_count_per_label_dict = {'asl': {'transformed'                   :  {},\n",
        "                                     'augmented'                     :  {},\n",
        "                                     'orig_mediapipe'                :  {},\n",
        "                                     'aug_mediapipe'                 :  {},\n",
        "                                     'consolidated_mediapipe'        :  {},\n",
        "                                     'transformed_images'            :  {},\n",
        "                                     'transformed_augmented_images'  :  {}\n",
        "                                    }\n",
        "                            }"
      ],
      "metadata": {
        "id": "qoPLcTzyTBcS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "These directory paths will be set in create directory structure method based on\n",
        "the sign language category being used.\n",
        "'''\n",
        "ROOT_DIR = os.getcwd()\n",
        "TEMP_DIR = os.path.join(ROOT_DIR, \"temp\")\n",
        "TEMP_AUG_DIR = os.path.join(ROOT_DIR, \"temp_aug\")\n",
        "\n",
        "STAGE_DATA_DIR = ''\n",
        "TRANSFORMED_DATA_DIR = ''\n",
        "AUGMENTED_DATA_DIR = ''\n",
        "CONSOLIDATED_MEDIAPIPE_DATA_DIR = ''\n",
        "ORIG_MEDIAPIPE_DATA_DIR = ''\n",
        "AUG_MEDIAPIPE_DATA_DIR = ''\n",
        "TRAIN_DATA_DIR = ''\n",
        "VAL_DATA_DIR = ''\n",
        "TEST_DATA_DIR = ''\n",
        "TRANSFORMED_IMAGE_DIR = ''\n",
        "TRANSFORMED_AUGMENTED_IMAGE_DIR = ''\n",
        "\n",
        "ASL_CATEGORY = 'asl'\n",
        "ISL_CATEGORY = 'isl'\n",
        "\n",
        "KFOLD = 5"
      ],
      "metadata": {
        "id": "Z_eGCOKt2lwe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Image and video augmentation variables.\n",
        "\"\"\"\n",
        "IMAGE_SIZE = 64\n",
        "# define the alpha and beta\n",
        "ALPHA = 1.5 # Contrast control\n",
        "BETA = 10 # Brightness control\n",
        "GAMMA = 1.0 # Gamma Control\n",
        "KERNEL_SIZE = 5\n",
        "ITERATIONS = 1\n",
        "SIZE = 9\n",
        "STD = 1.5\n",
        "MAX_FRAME = 10\n",
        "FPS = 30\n",
        "\n",
        "AUGMENT_FUNCTION_LIST = ['sharpen',\n",
        "                         'adj_brighntness',\n",
        "                         'adj_gamma',\n",
        "                         'flip',\n",
        "                         'rotate_90_or_180',\n",
        "                         'make_fast',\n",
        "                         'make_slow',\n",
        "                         'rotate_by_angle'\n",
        "                        ]"
      ],
      "metadata": {
        "id": "5ifE38UsKhip"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 video, 11 aug,\n",
        "# 12 * 90 * 1662\n",
        "# 12 * 90 * 1662 * 10 * 70"
      ],
      "metadata": {
        "id": "O2C4WdRI5Tbe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_data_dict = {'asl': {\n",
        "                          }\n",
        "                  }"
      ],
      "metadata": {
        "id": "C3-Dq4IrPf5k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.0001\n",
        "FAST_SPPED = 0.20\n",
        "SLOW_SPEED = 2"
      ],
      "metadata": {
        "id": "4dW2BeafNk6Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "kYUhpP8aPnMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set_seed"
      ],
      "metadata": {
        "id": "9kK14u-tSoik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed():\n",
        "    \"\"\"\n",
        "    This method sets the seed so that we can reproduce the results.\n",
        "    \"\"\"\n",
        "    np.random.seed(1234)\n",
        "    tf.random.set_seed(1234)\n",
        "    os.environ['PYTHONHASHSEED'] = '1234'\n",
        "    random.seed('1234')"
      ],
      "metadata": {
        "id": "nbeOu8BKPrMx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set_input_output_map_dict"
      ],
      "metadata": {
        "id": "qcnn6yCVn8mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_input_output_map_dict():\n",
        "    \"\"\"\n",
        "    This method sets up the dictionary input_output_map_dict.\n",
        "    \"\"\"\n",
        "    input_output_map_dict = {}\n",
        "    for label in label_list:\n",
        "        input_output_map_dict[label] = []"
      ],
      "metadata": {
        "id": "AlSKoXzGn-kv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check_file_count_in_a_directory"
      ],
      "metadata": {
        "id": "FpBDFUlXSz9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_file_count_in_a_directory(dir_path, file_extension):\n",
        "    \"\"\"\n",
        "    This function checks the file count in a directory.\n",
        "    The dir path does not include the end delimiter \"/\" or \"\\\".\n",
        "    \"\"\"\n",
        "    cmd_string = f'ls f{dir_path}/*.{file_extension} | wc -l'\n",
        "    file_count = int(subprocess.check_output(cmd_string, shell = True, text = True).strip())\n",
        "    return file_count"
      ],
      "metadata": {
        "id": "1A2HrjGiQptP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## copy_file_from_one_to_other"
      ],
      "metadata": {
        "id": "LTO5j4MoStS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_file_from_one_to_other(file_names, dest_path):\n",
        "    \"\"\"\n",
        "    This function moves chunks of files from source to destination path.\n",
        "    \"\"\"\n",
        "    os.system('cp -r ' + file_names + ' ' + dest_path)\n",
        "    print(check_file_count_in_a_directory(dest_path, 'jpeg'))"
      ],
      "metadata": {
        "id": "ZsjD4_DPPwa2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## process_copy_files"
      ],
      "metadata": {
        "id": "PX0cf8uiSvWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_copy_files(file_name_list, dest_path):\n",
        "    \"\"\"\"\n",
        "    This function processes moving files from one dir to the target path.\n",
        "    This is the master process to run actual moving in chunks.\n",
        "    file_name_list is list of fully qualified file names.\n",
        "    \"\"\"\n",
        "    process_chunk_size = 100\n",
        "    for idx in range(0, len(file_name_list), process_chunk_size):\n",
        "        if idx % 10000 == 0:\n",
        "            print(\"Processing index: \", idx)\n",
        "        copy_file_from_one_to_other(' '.join(file_name_list[idx : idx + process_chunk_size]), dest_path)"
      ],
      "metadata": {
        "id": "ExB6aLyUQnRf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_dir"
      ],
      "metadata": {
        "id": "lQucBk1-S26S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir(dir_path):\n",
        "    \"\"\"\n",
        "    Method to create directory\n",
        "    \"\"\"\n",
        "    #print(dir_path)\n",
        "    os.makedirs(dir_path, exist_ok = True)"
      ],
      "metadata": {
        "id": "0Yyhp3ZHQtLj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove_dir"
      ],
      "metadata": {
        "id": "iFG-0JRuS9kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_dir(dir_path):\n",
        "    \"\"\"\n",
        "    Method to remove directory\n",
        "    \"\"\"\n",
        "    dirpath = Path(dir_path)\n",
        "    if dirpath.exists() and dirpath.is_dir():\n",
        "        shutil.rmtree(dirpath)\n",
        "        #print(dir_path)\n",
        "        #shutil.rmtree(dir_path, ignore_errors=False, onerror=None)"
      ],
      "metadata": {
        "id": "N_lThAkZS_pd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove_temp_dir"
      ],
      "metadata": {
        "id": "CrQ90aTB_Xqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_temp_dir():\n",
        "    \"\"\"\n",
        "    This method removes temp directories.\n",
        "    \"\"\"\n",
        "    temp_dir_list = sorted(glob(f'{ROOT_DIR}/temp*'))\n",
        "    for temp_dir in temp_dir_list:\n",
        "        remove_dir(dir_path = temp_dir)\n",
        "\n",
        "    temp_aug_dir_list = sorted(glob(f'{ROOT_DIR}/temp_aug*'))\n",
        "    for temp_dir in temp_aug_dir_list:\n",
        "        remove_dir(dir_path = temp_dir)"
      ],
      "metadata": {
        "id": "nRq_PVEi_ar_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_dir_structure"
      ],
      "metadata": {
        "id": "iEsKKMyBS6t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir_structure(sign_language_category = \"asl\"):\n",
        "    \"\"\"\n",
        "    For staging folder, each label has two subfolders for image and video.\n",
        "    If corresponding to a label the data source is mp4 or a YouTube link, then\n",
        "    it will just have an mp4 video file present in the video folder. If the input\n",
        "    is a gif image, it will have the image data stored under image folder and\n",
        "    the corresponding video under video folder.\n",
        "    All the inputs will initially be stored under stage folder. Then the staged\n",
        "    videos will be transformed and will be placed under transformed data folder.\n",
        "    We will augment the video data and will store them in augmented video folder.\n",
        "    Now we will combine the transformed and augmented together in the\n",
        "    consolidated_data folder. At this stage, the data is ready for train-val-test split.\n",
        "\n",
        "    asl_dataset---------->|stage--------------->|label------>|image\n",
        "               |                                     |------>|video\n",
        "               |                                     |------>|video_aug\n",
        "               |                                     |------>|mediapipe\n",
        "               |\n",
        "               |--------->|transformed_data----------------->|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|augmented_data------------------->|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|orig_mediapipe_data-------------->|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|aug_mediapipe_data--------------->|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|consolidated_mediapipe_data------>|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|transformed_images--------------->|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|transformed_augmented_images----->|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|train---------------------------->|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|val------------------------------>|label\n",
        "               |\n",
        "               |\n",
        "               |--------->|test----------------------------->|label\n",
        "    \"\"\"\n",
        "    global STAGE_DATA_DIR\n",
        "    global TRANSFORMED_DATA_DIR\n",
        "    global AUGMENTED_DATA_DIR\n",
        "    global ORIG_MEDIAPIPE_DATA_DIR\n",
        "    global AUG_MEDIAPIPE_DATA_DIR\n",
        "    global CONSOLIDATED_MEDIAPIPE_DATA_DIR\n",
        "    global TRAIN_DATA_DIR\n",
        "    global VAL_DATA_DIR\n",
        "    global TEST_DATA_DIR\n",
        "    global TRANSFORMED_IMAGE_DIR\n",
        "    global TRANSFORMED_AUGMENTED_IMAGE_DIR\n",
        "\n",
        "    current_path = os.getcwd()\n",
        "    dataset_path = os.path.join(current_path, f'{sign_language_category}_dataset')\n",
        "    create_dir(dataset_path)\n",
        "\n",
        "    STAGE_DATA_DIR = os.path.join(dataset_path, 'stage')\n",
        "    TRANSFORMED_DATA_DIR = os.path.join(dataset_path, 'transformed_data')\n",
        "    AUGMENTED_DATA_DIR = os.path.join(dataset_path, 'augmented_data')\n",
        "    ORIG_MEDIAPIPE_DATA_DIR = os.path.join(dataset_path, 'orig_mediapipe_data')\n",
        "    AUG_MEDIAPIPE_DATA_DIR = os.path.join(dataset_path, 'aug_mediapipe_data')\n",
        "    CONSOLIDATED_MEDIAPIPE_DATA_DIR = os.path.join(dataset_path, 'consolidated_mediapipe_data')\n",
        "    TRAIN_DATA_DIR = os.path.join(dataset_path, 'train')\n",
        "    VAL_DATA_DIR = os.path.join(dataset_path, 'val')\n",
        "    TEST_DATA_DIR = os.path.join(dataset_path, 'test')\n",
        "    TRANSFORMED_IMAGE_DIR = os.path.join(dataset_path, 'transformed_images')\n",
        "    TRANSFORMED_AUGMENTED_IMAGE_DIR = os.path.join(dataset_path, 'transformed_augmented_images')\n",
        "\n",
        "    folder_category_list = ['stage',\n",
        "                            'transformed_data',\n",
        "                            'augmented_data',\n",
        "                            'orig_mediapipe_data',\n",
        "                            'aug_mediapipe_data',\n",
        "                            'consolidated_mediapipe_data',\n",
        "                            'transformed_images',\n",
        "                            'transformed_augmented_images',\n",
        "                            'train',\n",
        "                            'test',\n",
        "                            'val'\n",
        "                           ]\n",
        "    for folder_category in folder_category_list:\n",
        "        #print(folder_category)\n",
        "        folder_category_path = os.path.join(dataset_path, folder_category)\n",
        "        create_dir(folder_category_path)\n",
        "\n",
        "        for label in label_list:\n",
        "            label_path = os.path.join(folder_category_path, label)\n",
        "            create_dir(label_path)\n",
        "            # if the folder type is stage, then only image files are retained.\n",
        "            # Our training data are video, so we will be storing video data\n",
        "            # under input, train, test and val folder structure.\n",
        "            if folder_category == 'stage':\n",
        "                label_image_path = os.path.join(label_path, 'image')\n",
        "                create_dir(label_image_path)\n",
        "                label_video_path = os.path.join(label_path, 'video')\n",
        "                create_dir(label_video_path)\n",
        "                label_video_aug_path = os.path.join(label_path, 'video_aug')\n",
        "                create_dir(label_video_aug_path)\n",
        "                label_mediapipe_path = os.path.join(label_path, 'mediapipe')\n",
        "                create_dir(label_mediapipe_path)"
      ],
      "metadata": {
        "id": "5TRoRtYPQwZq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_shuffle_indices"
      ],
      "metadata": {
        "id": "NpPybUHbUdII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_shuffle_indices(np_array):\n",
        "    \"\"\"\n",
        "    This method returns the shuffled indices.\n",
        "    \"\"\"\n",
        "    return tf.random.shuffle(tf.range(tf.shape(np_array)[0], dtype = tf.int32))"
      ],
      "metadata": {
        "id": "hyemTTDaQ3Hr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read_config_json"
      ],
      "metadata": {
        "id": "6RfMkEeEUfBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_config_json(json_file):\n",
        "    \"\"\"\n",
        "    This method reads json file from google drive and returns the config details.\n",
        "    \"\"\"\n",
        "    f = open(f\"{os.environ['GDRIVE_CONFIG_DIR']}/{json_file}\")\n",
        "    config_data = json.load(f)\n",
        "    return config_data"
      ],
      "metadata": {
        "id": "7gZUvcSPRFEA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read_data_from_private_github_repo"
      ],
      "metadata": {
        "id": "EBcJ9HhVUjlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_from_private_github_repo(label, sign_language_category = 'asl'):\n",
        "    \"\"\"\n",
        "    This method reads confog details, required to read data from\n",
        "    private github repo and reads data from the private github repo.\n",
        "    \"\"\"\n",
        "    config_file = f\"{sign_language_category}_config.json\"\n",
        "    config_data = read_config_json(config_file)\n",
        "    #print(config_data)\n",
        "\n",
        "    # Variables\n",
        "    prefix = config_data['prefix']\n",
        "    org = config_data['org']\n",
        "    repo = config_data['repo']\n",
        "    branch = config_data['branch']\n",
        "    folder = config_data[f'{sign_language_category}_folder']\n",
        "    file = f\"{label}.txt\"\n",
        "    url = f\"{prefix}/{org}/{repo}/{branch}/{folder}/{file}\"\n",
        "    #print(url)\n",
        "\n",
        "    # Headers setup\n",
        "    headers = CaseInsensitiveDict()\n",
        "    headers[\"Authorization\"] = \"token \" + config_data['token']\n",
        "\n",
        "    # Execute and view status\n",
        "    req_response = requests.get(url, headers = headers)\n",
        "    if req_response.status_code == 200:\n",
        "        file_list = [file for file in req_response.content.decode('UTF-8').split(\"\\n\") if file != '']\n",
        "        return [file_list, len(file_list)]\n",
        "    else:\n",
        "        print(\"Request failed!\")\n",
        "        return -1"
      ],
      "metadata": {
        "id": "Hf90w-yoRO4S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## push_data_to_private_github_repo"
      ],
      "metadata": {
        "id": "i8EKNL6HAmC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def push_data_to_private_github_repo(sign_language_category, label, file):\n",
        "    \"\"\"\n",
        "    This method reads confog details, required to read data from\n",
        "    private github repo and pushes data to a private github repo.\n",
        "    \"\"\"\n",
        "    config_file = f\"{sign_language_category}_config.json\"\n",
        "    config_data = read_config_json(config_file)\n",
        "    #print(config_data)\n",
        "\n",
        "    # Variables\n",
        "    prefix = config_data['prefix']\n",
        "    org = config_data['org']\n",
        "    repo = config_data['repo']\n",
        "    branch = config_data['branch']\n",
        "    folder = config_data[f'{sign_language_category}_folder']\n",
        "    token = config_data['token']\n",
        "    github = Github(token)\n",
        "    github_repo = github.get_repo(f'{org}/{repo}')\n",
        "    with open(file, 'r') as file:\n",
        "        data = file.read()\n",
        "    now = datetime.now()\n",
        "    ts = now.strftime(\"%Y%m%d%H%M%S\")\n",
        "    commit_messge = f\"{file} uploaded to {repo} at {ts}\"\n",
        "    try:\n",
        "        file = repo.get_contents(file)\n",
        "    except:\n",
        "        file = '-1'\n",
        "\n",
        "    # If the file is new, it will be created, else it will be updated.\n",
        "    if file == '-1':\n",
        "        github_repo.create_file(file, commit_messge, data, branch = branch)\n",
        "    else:\n",
        "        repo.update_file(file, commit_messge, data, branch='main', sha = file.sha)"
      ],
      "metadata": {
        "id": "s_mKwY8dAuPF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_video_file_stats"
      ],
      "metadata": {
        "id": "UfEew3QvUpHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_file_stats(video_file_name):\n",
        "    cap = cv2.VideoCapture(video_file_name)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    '''\n",
        "    print(f\"Frame count : {frame_count}\")\n",
        "    print(f\"Frame height : {height}\")\n",
        "    print(f\"Frame width : {width}\")\n",
        "    print(f\"Frame fps : {fps}\")\n",
        "    '''\n",
        "    return frame_count, height, width, fps"
      ],
      "metadata": {
        "id": "92FiBHLfnlkX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_video_file_stats(video_file_name = f'{TRANSFORMED_DATA_DIR}/hello/hello_399a9050_8026_11ee_b36e_0242ac1c000c.mp4')"
      ],
      "metadata": {
        "id": "P5ifR3F4Hgm9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download_video_from_youtube"
      ],
      "metadata": {
        "id": "XOUqHROcUr6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video_from_youtube(video_url, label, dir_path):\n",
        "    \"\"\"\n",
        "    This method downloads mp4 file from a gien youtube url and\n",
        "    saves them in the given path.\n",
        "    \"\"\"\n",
        "    yt = YouTube(video_url)\n",
        "    video_file_name = label + \"_\" + str(uuid.uuid1()).replace(\"-\",\"_\") + \".mp4\"\n",
        "    yt = yt.streams.filter(progressive = True, file_extension = 'mp4').order_by('resolution').desc().first()\n",
        "    video_file_path = os.path.join(dir_path, label, 'video')\n",
        "    yt.download(video_file_path, filename = video_file_name)\n",
        "    input_output_map_dict[label].append((video_url, video_file_name))"
      ],
      "metadata": {
        "id": "YprRc_2MnoXF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download_video_from_youtube_and_trim"
      ],
      "metadata": {
        "id": "nksutq-buu0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video_from_youtube_and_trim(video_url, label, start_ts, end_ts, dir_path):\n",
        "    \"\"\"\n",
        "    This method downloads mp4 file from a gien youtube url and\n",
        "    saves them in the given path.\n",
        "    \"\"\"\n",
        "    yt = YouTube(video_url)\n",
        "    video_file_name = label + \"_\" + str(uuid.uuid1()).replace(\"-\",\"_\") + \".mp4\"\n",
        "    temp_video_file_name = \"temp_\" + video_file_name\n",
        "    yt = yt.streams.filter(progressive = True, file_extension = 'mp4').order_by('resolution').desc().first()\n",
        "    video_file_path = os.path.join(dir_path, label, 'video')\n",
        "    #print(video_file_name)\n",
        "    #print(temp_video_file_name)\n",
        "    #print(video_file_path)\n",
        "    yt.download(video_file_path, filename = temp_video_file_name)\n",
        "    qualified_temp_video_file_name = os.path.join(dir_path, label, 'video', temp_video_file_name)\n",
        "    qualified_video_file_name = os.path.join(dir_path, label, 'video', video_file_name)\n",
        "    # Cropping video data, suppressing audio and subtitle\n",
        "    #!ffmpeg -i {qualified_temp_video_file_name} -map 0 -map -0:a -sn -vf trim={start_ts}:{end_ts} {qualified_video_file_name}\n",
        "    # Deleting temp video file once the crop is done\n",
        "    #!rm -rf {qualified_temp_video_file_name}\n",
        "    clip = mved.VideoFileClip(qualified_temp_video_file_name).without_audio()\n",
        "    clip = clip.subclip(start_ts, end_ts)\n",
        "    video_file_name = label + \"_\" + str(uuid.uuid1()).replace(\"-\",\"_\") + \".mp4\"\n",
        "    qualified_video_file_name = os.path.join(dir_path, label, 'video', video_file_name)\n",
        "    clip.write_videofile(qualified_video_file_name)\n",
        "    !rm -rf __temp__.mp4\n",
        "    !rm -rf {temp_video_file_name}\n",
        "    input_output_map_dict[label].append((video_url, video_file_name))"
      ],
      "metadata": {
        "id": "g6RdH_0TuvbD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download_video_from_url"
      ],
      "metadata": {
        "id": "Raba3mC_Ut8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video_from_url(url_link, label, dir_path):\n",
        "    \"\"\"\n",
        "    This methid downloads video from the given url link and\n",
        "    saves them in the given path.\n",
        "    \"\"\"\n",
        "    url = url_link\n",
        "    query_parameters = {\"downloadformat\": \"mp4\"}\n",
        "    response = requests.get(url, params = query_parameters, stream = True)\n",
        "    print(response.url, response.ok, response.status_code)\n",
        "    if response.status_code == 200:\n",
        "        # how%20(alternate) handling\n",
        "        download_file_name = response.url.split(\"/\")[-1].split(\"?\")[0].replace(\"%\",\"_\").replace(\"(\",\"\").replace(\")\",\"\")\n",
        "        target_path = os.path.join(dir_path, label, 'video')\n",
        "        video_file_name = label + \"_\" + str(uuid.uuid1()).replace(\"-\",\"_\") + \".mp4\"\n",
        "        with open(download_file_name, mode = \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size = 10 * 1024):\n",
        "                file.write(chunk)\n",
        "        !mv {download_file_name} {video_file_name}\n",
        "        !mv {video_file_name} {target_path}\n",
        "        input_output_map_dict[label].append((url_link, video_file_name))"
      ],
      "metadata": {
        "id": "si__pwV3nrdV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download_video_from_url_and_trim"
      ],
      "metadata": {
        "id": "iOm3O03zwJ_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video_from_url_and_trim(url_link, label, start_ts, end_ts, dir_path):\n",
        "    \"\"\"\n",
        "    This methid downloads video from the given url link and\n",
        "    saves them in the given path.\n",
        "    \"\"\"\n",
        "    url = url_link\n",
        "    query_parameters = {\"downloadformat\": \"mp4\"}\n",
        "    response = requests.get(url, params = query_parameters, stream = True)\n",
        "    print(response.url, response.ok, response.status_code)\n",
        "    if response.status_code == 200:\n",
        "        download_file_name = response.url.split(\"/\")[-1].split(\"?\")[0]\n",
        "        target_path = os.path.join(dir_path, label, 'video')\n",
        "        temp_video_file_name = \"temp_\" + label + \"_\" + str(uuid.uuid1()).replace(\"-\",\"_\") + \".mp4\"\n",
        "        video_file_name = temp_video_file_name[5:]\n",
        "        with open(download_file_name, mode = \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size = 10 * 1024):\n",
        "                file.write(chunk)\n",
        "\n",
        "        # Rename downloaded file to a temp video file\n",
        "        #print(f\"Rename downloaded file {download_file_name} to a temp video file {temp_video_file_name}\")\n",
        "        !mv {download_file_name} {temp_video_file_name}\n",
        "\n",
        "        # Crop the temp video file to actual video file\n",
        "        #print(f\"Cropping the temp video file {temp_video_file_name} to actual video file {video_file_name}\")\n",
        "        #print(f\"!ffmpeg -i {temp_video_file_name} -vf trim = {start_ts}:{end_ts} {video_file_name}\")\n",
        "        # Cropping video data, suppressing audio and subtitle\n",
        "        #!ffmpeg -i {temp_video_file_name} -map 0 -map -0:a -sn -vf trim={start_ts}:{end_ts} {video_file_name}\n",
        "\n",
        "        clip = mved.VideoFileClip(temp_video_file_name).without_audio()\n",
        "        clip = clip.subclip(start_ts, end_ts)\n",
        "        qualified_video_file_name = os.path.join(target_path, video_file_name)\n",
        "        clip.write_videofile(qualified_video_file_name)\n",
        "        !rm -rf __temp__.mp4\n",
        "\n",
        "        # Move the actual video file to target\n",
        "        #print(f\"Moving {video_file_name} to {target_path}\")\n",
        "        #!mv {video_file_name} {target_path}\n",
        "\n",
        "        # Remove the temp video file\n",
        "        #print(f\"Removing the temp video file {temp_video_file_name}\")\n",
        "        !rm -rf {temp_video_file_name}\n",
        "        input_output_map_dict[label].append((url_link, video_file_name))"
      ],
      "metadata": {
        "id": "cPwpkZb9wKTv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load_video"
      ],
      "metadata": {
        "id": "ss4ntnBaUwjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(path, max_frames = 0, resize = (IMAGE_SIZE, IMAGE_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ],
      "metadata": {
        "id": "5vxwqw6jnuAd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display_video"
      ],
      "metadata": {
        "id": "5p3ksasAZQTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video(qualified_video_file_name):\n",
        "    \"\"\"\n",
        "    This method displays video on the screen.\n",
        "    \"\"\"\n",
        "    mp4 = open(qualified_video_file_name,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "   \"\"\" % data_url)"
      ],
      "metadata": {
        "id": "n0fEePXlZTUu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load_images_from_folder"
      ],
      "metadata": {
        "id": "n6mlT_SZUzZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder, file_extension):\n",
        "    images = []\n",
        "    for filename in [file for file in sorted(os.listdir(folder)) if f'.{file_extension}' in file]:\n",
        "        image = cv2.imread(os.path.join(folder, filename), 0)\n",
        "        if image is not None:\n",
        "            images.append(image)\n",
        "    return np.array(images) / 255"
      ],
      "metadata": {
        "id": "O9x_4EUwnx2s"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove_background"
      ],
      "metadata": {
        "id": "zyJxU_u1U1Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_background(images):\n",
        "    return [remove(image) for image in images]"
      ],
      "metadata": {
        "id": "bjXAnnKvnz__"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert_to_grayscale"
      ],
      "metadata": {
        "id": "rePHYECfU3NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_grayscale(images):\n",
        "    return [cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in images]"
      ],
      "metadata": {
        "id": "aZnLiqnKn4s7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resize_images"
      ],
      "metadata": {
        "id": "uwsBnMoSU5zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_images(images, image_size):\n",
        "    return [cv2.resize(image, (image_size, image_size), interpolation= cv2.INTER_LINEAR) for image in images]"
      ],
      "metadata": {
        "id": "86mJZ_UVn7Wj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rescale_images"
      ],
      "metadata": {
        "id": "QBFNPu8wU8Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale_images(images):\n",
        "    return [image / 255 for image in images]"
      ],
      "metadata": {
        "id": "RYsb4h8An-rD"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check_image"
      ],
      "metadata": {
        "id": "AXM37DJeTAOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_image(img, image_size = IMAGE_SIZE):\n",
        "    \"\"\"\n",
        "    Checks if the given image is just white or black.\n",
        "    \"\"\"\n",
        "    img = cv2.resize(img, (image_size, image_size))\n",
        "    if np.sum(np.ravel(img)) == image_size ^ 2 or np.sum(img) == 0:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "Q69hKhRqAic0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## blur_images"
      ],
      "metadata": {
        "id": "CyjmEgasU-J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def blur_images(images, size):\n",
        "    return [cv2.GaussianBlur(image, (size, size), 0) for image in images]"
      ],
      "metadata": {
        "id": "4BhxkA2toApQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add_periodic_noise"
      ],
      "metadata": {
        "id": "Tc6_InffxOD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_periodic_noise_to_grayscale_imae(images, amplitude=25, frequency=0.1):\n",
        "    \"\"\"\n",
        "    Add periodic noise to an image.\n",
        "    \"\"\"\n",
        "    image_list = []\n",
        "    for image in images:\n",
        "        if not check_image(image, IMAGE_SIZE):\n",
        "            row, col = image.shape\n",
        "            x = np.arange(col)\n",
        "            y = amplitude * np.sin(2 * np.pi * frequency * x / col)\n",
        "            noise = np.tile(y, (row, 1))\n",
        "            noisy = image + noise\n",
        "            noisy = np.clip(noisy, 0, 255)  # Clip values to be in the [0, 255] range\n",
        "            noisy = noisy.astype(np.uint8)\n",
        "            image_list.append(noisy)\n",
        "        else:\n",
        "            image_list.append(image)\n",
        "    return image_list"
      ],
      "metadata": {
        "id": "k-mhMJFFxPtm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add_random_noise"
      ],
      "metadata": {
        "id": "EqdnxWC69r7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_random_noise(images, mode):\n",
        "    return [skimage.util.random_noise(images / 255, mode=mode) for image in images]"
      ],
      "metadata": {
        "id": "XcuW1Hz79pnP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sharpen_images"
      ],
      "metadata": {
        "id": "OM7l6g1mVAD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sharpen_images(images):\n",
        "    sharpen_filter = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "    return [cv2.filter2D(image, -1, sharpen_filter) for image in images]"
      ],
      "metadata": {
        "id": "J2frohIuoBPS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adjust_contrast_brightness"
      ],
      "metadata": {
        "id": "eOuNNjB8VEDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_contrast_brightness(images, alpha = ALPHA, beta = BETA):\n",
        "    return [cv2.convertScaleAbs(image, alpha=alpha, beta=beta) for image in images]"
      ],
      "metadata": {
        "id": "Bx_IYsnKoGE3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adjust_gamma"
      ],
      "metadata": {
        "id": "QUtLvG1WVF1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_gamma(images, gamma = GAMMA):\n",
        "    return [np.array(255 * (image / 255) ** gamma, dtype = 'uint8') for image in images]"
      ],
      "metadata": {
        "id": "ASt7z63noJNU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## apply_log_transform_image"
      ],
      "metadata": {
        "id": "xkbeH56LVIjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_log_transform_image(image):\n",
        "    # Apply log transform.\n",
        "    c = 255/(np.log(1 + np.max(image)))\n",
        "    log_transformed = c * np.log(1 + image)\n",
        "\n",
        "    # Specify the data type.\n",
        "    log_transformed = np.array(log_transformed, dtype = np.uint8)\n",
        "    return log_transformed\n",
        "\n",
        "def apply_log_transform_imagesets(images):\n",
        "    image_list = []\n",
        "    for image in images:\n",
        "        if not check_image(image, IMAGE_SIZE):\n",
        "            image_list.append(apply_log_transform_image(image))\n",
        "        else:\n",
        "            image_list.append(image)\n",
        "    return image_list\n",
        "    #return [apply_log_transform_image(image) for image in images]"
      ],
      "metadata": {
        "id": "wkVDR3WroLe6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enhance_images"
      ],
      "metadata": {
        "id": "7U4ROyLmVO25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enhance_images(images, image_size = 8, clip_limit = 2.0):\n",
        "    clahe = cv2.createCLAHE(clipLimit = clip_limit, tileGridSize=(image_size, image_size))\n",
        "    return [clahe.apply(image) for image in images]"
      ],
      "metadata": {
        "id": "51zTV2a-oOw9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rotate_images"
      ],
      "metadata": {
        "id": "BcsEcX-LVNKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_images(images, rotation_type):\n",
        "    if rotation_type == 1:\n",
        "        return [cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE) for image in images]\n",
        "    elif rotation_type == 2:\n",
        "        return [cv2.rotate(image, cv2.ROTATE_180) for image in images]\n",
        "    elif rotation_type == 3:\n",
        "        return [cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE) for image in images]"
      ],
      "metadata": {
        "id": "z1bw4Y5MoQv6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## erode_images"
      ],
      "metadata": {
        "id": "RM3SxA-MVLq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def erode_images(images, kernel_size = KERNEL_SIZE, iterations = ITERATIONS):\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    return [cv2.erode(image, kernel, iterations) for image in images]"
      ],
      "metadata": {
        "id": "7ymOIAtVoS-K"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dilate_images"
      ],
      "metadata": {
        "id": "f80XD9csURVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dilate_images(images, kernel_size = KERNEL_SIZE, iterations = ITERATIONS):\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    return [cv2.dilate(image, kernel, iterations) for image in images]"
      ],
      "metadata": {
        "id": "VtG-Y0X3oWVD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## equalize_hist_images"
      ],
      "metadata": {
        "id": "D9Qn80azUP2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equalize_hist_images(images):\n",
        "    image_list = []\n",
        "    return [cv2.equalizeHist(image) for image in images]"
      ],
      "metadata": {
        "id": "ylV0n856oYVK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rotate_image_by_angle"
      ],
      "metadata": {
        "id": "Dm0qhD3UUOYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image_by_angle(images, angle):\n",
        "    return resize_images([ndimage.rotate(image, angle) for image in images], IMAGE_SIZE)"
      ],
      "metadata": {
        "id": "hjXa-KpgoaeY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## flip_images"
      ],
      "metadata": {
        "id": "CSJ-uIU2UMxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flip_images(images, flip_ind):\n",
        "    return [cv2.flip(image, flip_ind) for image in images]"
      ],
      "metadata": {
        "id": "FnqfTbmJoddu"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove_texture"
      ],
      "metadata": {
        "id": "tww0fVoYUDlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_2d_gaussian(size = SIZE, std = STD):\n",
        "    gaussian_1d = signal.gaussian(size,std=std)\n",
        "    gaussian_2d = np.outer(gaussian_1d, gaussian_1d)\n",
        "    gaussian_2d = gaussian_2d/(gaussian_2d.sum())\n",
        "    return gaussian_2d\n",
        "\n",
        "def remove_texture(image):\n",
        "    low_pass_gaussain = create_2d_gaussian(15, 8)\n",
        "    img_less_texture = convolve(image, low_pass_gaussain)\n",
        "    return img_less_texture"
      ],
      "metadata": {
        "id": "wHe8c9FFogIK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_edge_feature"
      ],
      "metadata": {
        "id": "QdOLDCrYUCKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_edge_feature(images, threshold1, threshold2):\n",
        "    return [cv2.Canny(remove_texture(remove_texture(image)), threshold1 = threshold1, threshold2 = threshold2) for image in images]"
      ],
      "metadata": {
        "id": "RJ_bk3dWoju0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_prewitth_kernel_feature"
      ],
      "metadata": {
        "id": "w72ybfTHUAog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prewitth_kernel_feature(images):\n",
        "    return [prewitt_h(image) for image in images]"
      ],
      "metadata": {
        "id": "YpWNXWxMom22"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_prewittv_kernel_feature"
      ],
      "metadata": {
        "id": "z0IaAcp1T_NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prewittv_kernel_feature(images):\n",
        "    return [prewitt_v(image) for image in images]"
      ],
      "metadata": {
        "id": "lEezcS4jopVs"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_canny_feature"
      ],
      "metadata": {
        "id": "QzVXcBkfT69o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_canny_feature(images):\n",
        "    return [cv2.Canny(image, 100, 200) for image in images]"
      ],
      "metadata": {
        "id": "46HqZ6Gaorp9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_sobel_feature"
      ],
      "metadata": {
        "id": "TyI6V3foT5hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sobel_feature(images):\n",
        "    return [filters.sobel(image) for image in images]"
      ],
      "metadata": {
        "id": "TMEgcB-_ze5j"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_contour_feature"
      ],
      "metadata": {
        "id": "rDFRqtyopNnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_contour_feature(images):\n",
        "    return [np.array(Image.fromarray(np.uint8(image)).filter(ImageFilter.CONTOUR)) for image in images]"
      ],
      "metadata": {
        "id": "vO5aGIh2zhBc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_edge1_feature"
      ],
      "metadata": {
        "id": "sHax1I3oT4Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_edge1_feature(images):\n",
        "    return [np.array(Image.fromarray(np.uint8(image)).filter(ImageFilter.EDGE_ENHANCE)) for image in images]"
      ],
      "metadata": {
        "id": "7FD3DuqizjVz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_edge2_feature"
      ],
      "metadata": {
        "id": "8UGK_FLFT1fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_edge2_feature(images):\n",
        "    return [np.array(Image.fromarray(np.uint8(image)).filter(ImageFilter.EDGE_ENHANCE_MORE)) for image in images]"
      ],
      "metadata": {
        "id": "TXClPw_1zlWX"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## crop_center_square"
      ],
      "metadata": {
        "id": "lG18xbFwT0B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
      ],
      "metadata": {
        "id": "_HBcTjwwzsfa"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read_gif"
      ],
      "metadata": {
        "id": "fuvfM5BeTyTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_gif(file_name, dir_path):\n",
        "    \"\"\"\n",
        "    This method reads a qualified gif file and saves each frame in png format.\n",
        "    \"\"\"\n",
        "    img = Image.open(file_name)\n",
        "    gif_file_name = file_name.split(\"/\")[-1].split(\".\")[0]\n",
        "    index = 1\n",
        "    for frame in ImageSequence.Iterator(img):\n",
        "        frame_file_name = os.path.join(dir_path, f\"{gif_file_name}_frame_{index}.png\")\n",
        "        frame.save(frame_file_name)\n",
        "        index += 1"
      ],
      "metadata": {
        "id": "A92ThbdYzpNt"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display_video"
      ],
      "metadata": {
        "id": "sYVBsKWSTsQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video(qualified_video_file_name):\n",
        "    \"\"\"\n",
        "    Method to display video\n",
        "    \"\"\"\n",
        "    url = upload_public(qualified_video_file_name)\n",
        "    HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
      ],
      "metadata": {
        "id": "Sc2ubGDjuHBv"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## build_label_dict"
      ],
      "metadata": {
        "id": "ySsX79e5UnAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_label_dict(label, sign_language_category = 'asl'):\n",
        "    \"\"\"\n",
        "    This method builds the label dictionary for the given label as key\n",
        "    and file list as the value.\n",
        "    Ex. label_data_dict['asl']['hello'] = [[file1, file2...], count of files]\n",
        "    \"\"\"\n",
        "    label_data_dict[sign_language_category][label] = read_data_from_private_github_repo(label,\n",
        "                                                                                        sign_language_category\n",
        "                                                                                       )"
      ],
      "metadata": {
        "id": "09sy5i5knjNW"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prepare_input_dataset_for_label"
      ],
      "metadata": {
        "id": "GL9mO7iDTqpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input_dataset_for_label(label, stage_dir_path, sign_language_category = 'asl'):\n",
        "     \"\"\"\n",
        "     This methiod prepares the input dataset and stores them in the given\n",
        "     directory path. The input file can be mp4, gif or a Youtube video.\n",
        "     \"\"\"\n",
        "     # Below function returns file list and file count for a label.\n",
        "     # label_data_dict['asl']['hello'] = [[file1, file2...], count of files]\n",
        "     build_label_dict(sign_language_category = sign_language_category,\n",
        "                      label = label)\n",
        "     file_list = label_data_dict[sign_language_category][label][0]\n",
        "     file_count = label_data_dict[sign_language_category][label][1]\n",
        "     print(f\"There are {file_count} files for label {label}\")\n",
        "     for file in file_list:\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"--------------------------------\")\n",
        "        print(f\"Processing {file}\")\n",
        "        print(\"--------------------------------\")\n",
        "        if file[-3:] == 'gif':\n",
        "            print(\"Input file is in gif format.\")\n",
        "            #Forming the image file name\n",
        "            image_file_name = label + \"_\" + str(uuid.uuid1()).replace(\"-\",\"_\") + \".gif\"\n",
        "            video_file_name = label + \"_\" + str(uuid.uuid1()).replace(\"-\",\"_\") + \".mp4\"\n",
        "            qualified_image_file_name = os.path.join(stage_dir_path,\n",
        "                                                     label,\n",
        "                                                     'image',\n",
        "                                                     image_file_name\n",
        "                                                    )\n",
        "            #print(qualified_image_file_name)\n",
        "            # Reading the gif file from the given url\n",
        "            try:\n",
        "                imdata = urllib.request.urlopen(file).read()\n",
        "                imbytes = bytearray(imdata)\n",
        "\n",
        "                # Writing the image file under <>/image path\n",
        "                open(qualified_image_file_name, \"wb+\").write(imdata)\n",
        "\n",
        "                # Converting the gif file to mp4\n",
        "                clip = mved.VideoFileClip(qualified_image_file_name)\n",
        "                qualified_video_file_name = os.path.join(stage_dir_path,\n",
        "                                                         label,\n",
        "                                                         'video',\n",
        "                                                         video_file_name\n",
        "                                                        )\n",
        "                #print(qualified_video_file_name)\n",
        "                # Writing the video file under <>/video path\n",
        "                clip.write_videofile(qualified_video_file_name)\n",
        "\n",
        "            except:\n",
        "                print(f\"Error in processing {file}\")\n",
        "\n",
        "        elif '|' in file and ':' in file and file.split(\"|\")[0][-3:] == 'mp4' and 'youtube.com' not in file:\n",
        "\n",
        "            print(\"Input file is an embedded video from a site other than youtube and video needs to be cropped.\")\n",
        "            # Downloading video from a given url other than youtube and the file needs to be trimmed.\n",
        "            file_name = file.split(\"|\")[0]\n",
        "            start_ts = int(file.split(\"|\")[1].split(\":\")[0])\n",
        "            end_ts = int(file.split(\"|\")[1].split(\":\")[1])\n",
        "            try:\n",
        "                download_video_from_url_and_trim(url_link = file_name,\n",
        "                                                 label = label,\n",
        "                                                 start_ts = start_ts,\n",
        "                                                 end_ts = end_ts,\n",
        "                                                 dir_path = stage_dir_path)\n",
        "            except:\n",
        "                print(f\"Error in processing {file}\")\n",
        "\n",
        "        elif file[-3:] == 'mp4' and 'youtube.com' not in file:\n",
        "\n",
        "            print(\"Input file is an embedded video from a site other than youtube.\")\n",
        "            # Downloading video from a given url other than youtube\n",
        "            try:\n",
        "                download_video_from_url(url_link = file,\n",
        "                                        label = label,\n",
        "                                        dir_path = stage_dir_path)\n",
        "            except:\n",
        "                print(f\"Error in processing {file}\")\n",
        "\n",
        "        elif '|' in file and ':' in file and \"https://www.youtube.com/shorts/\" in file:\n",
        "\n",
        "            print(\"Input file is an youtube shorts video and needs to be cropped.\")\n",
        "            # The youtube shorts video needs to be trimmed.\n",
        "            file_name = file.split(\"|\")[0]\n",
        "            start_ts = int(float(file.split(\"|\")[1].split(\":\")[0]))\n",
        "            end_ts = int(float(file.split(\"|\")[1].split(\":\")[1]))\n",
        "            try:\n",
        "                download_video_from_youtube_and_trim(video_url = file_name,\n",
        "                                                     label = label,\n",
        "                                                     start_ts = start_ts,\n",
        "                                                     end_ts = end_ts,\n",
        "                                                     dir_path = stage_dir_path)\n",
        "            except:\n",
        "                print(f\"Error in processing {file}\")\n",
        "\n",
        "        elif '|' in file and ':' in file and \"youtube.com\" in file:\n",
        "\n",
        "            print(\"Input file is an youtube video and needs to be cropped.\")\n",
        "            # The youtube video needs to be trimmed.\n",
        "            file_name = file.split(\"|\")[0]\n",
        "            start_ts = int(float(file.split(\"|\")[1].split(\":\")[0]))\n",
        "            end_ts = int(float(file.split(\"|\")[1].split(\":\")[1]))\n",
        "            try:\n",
        "                download_video_from_youtube_and_trim(video_url = file_name,\n",
        "                                                     label = label,\n",
        "                                                     start_ts = start_ts,\n",
        "                                                     end_ts = end_ts,\n",
        "                                                     dir_path = stage_dir_path)\n",
        "            except:\n",
        "                print(f\"Error in processing {file}\")\n",
        "\n",
        "        elif 'https://www.youtube.com/shorts/' in file:\n",
        "\n",
        "            print(\"Input file is an youtube shorts video.\")\n",
        "            # Downloading video from youtube shorts\n",
        "            try:\n",
        "                download_video_from_youtube(video_url = file,\n",
        "                                            label = label,\n",
        "                                            dir_path = stage_dir_path)\n",
        "            except:\n",
        "                print(f\"Error in processing {file}\")\n",
        "\n",
        "        elif 'watch?v=' in file and \"youtube.com\" in file:\n",
        "\n",
        "            print(\"Input file is an youtube video.\")\n",
        "            # Downloading video from youtube\n",
        "            try:\n",
        "                download_video_from_youtube(video_url = file,\n",
        "                                            label = label,\n",
        "                                            dir_path = stage_dir_path)\n",
        "            except:\n",
        "                print(f\"Error in processing {file}\")"
      ],
      "metadata": {
        "id": "1AbuGAE-z0zW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prepare_input_dataset_for_label_set"
      ],
      "metadata": {
        "id": "Y0t6vwR0TnLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input_dataset_for_label_set(sign_language_category = \"asl\",\n",
        "                                        stage_dir_path = STAGE_DATA_DIR,\n",
        "                                        label_set = label_list\n",
        "                                       ):\n",
        "     [prepare_input_dataset_for_label(label = label,\n",
        "                                      stage_dir_path = stage_dir_path,\n",
        "                                      sign_language_category = sign_language_category\n",
        "                                     )\n",
        "      for label in label_set\n",
        "     ]"
      ],
      "metadata": {
        "id": "U3f2lgR_0AKG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mediapipe_detection"
      ],
      "metadata": {
        "id": "nY2f-PNhTcAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                  # Image is no longer writeable\n",
        "    results = model.process(image)                 # Make prediction\n",
        "    image.flags.writeable = True                   # Image is now writeable\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "    return image, results"
      ],
      "metadata": {
        "id": "kYXtnK-a1VVg"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## draw_landmarks"
      ],
      "metadata": {
        "id": "5pBRXrjCTaU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image, results):\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face contours\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face tesslation\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
      ],
      "metadata": {
        "id": "0bRiVPDU1bpV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## draw_styled_landmarks"
      ],
      "metadata": {
        "id": "XBb3CQ1WTYzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             )\n",
        "\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             )\n",
        "\n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw right hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                             )"
      ],
      "metadata": {
        "id": "mzVCqs791gvr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display_video_with_media_pipe"
      ],
      "metadata": {
        "id": "rRxBv2UOTV3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video_with_media_pipe(video_name):\n",
        "    cap = cv2.VideoCapture(video_name)\n",
        "    # Set mediapipe model\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        while cap.isOpened():\n",
        "\n",
        "            # Read feed\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if frame is None:\n",
        "                break\n",
        "\n",
        "            # Make detections\n",
        "            image, results = mediapipe_detection(frame, holistic)\n",
        "            print(results)\n",
        "\n",
        "            # Draw landmarks\n",
        "            draw_styled_landmarks(image, results)\n",
        "\n",
        "            # Show to screen\n",
        "            cv2_imshow(image)\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    return image, results"
      ],
      "metadata": {
        "id": "Og2ghYan1lS4"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extract_keypoints"
      ],
      "metadata": {
        "id": "0JxpkMs6TSPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, face, lh, rh])"
      ],
      "metadata": {
        "id": "FA96mlJK1pDH"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## landmark_min_max_norm"
      ],
      "metadata": {
        "id": "FHjxQ_7wTQm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def landmark_min_max_norm(np_array, viz_flag, marker):\n",
        "    #print(np_array == None, marker, viz_flag)\n",
        "    #print(marker == 'L' and np_array == None)\n",
        "    if marker == 'P' and np_array == None:\n",
        "        return np.zeros(132)\n",
        "    elif marker == 'P' and np_array != None:\n",
        "        landmark = np_array.landmark\n",
        "\n",
        "    if marker == 'L' and np_array == None:\n",
        "        return np.zeros(63)\n",
        "    elif marker == 'L' and np_array != None:\n",
        "        landmark = np_array.landmark\n",
        "\n",
        "    if marker == 'R' and np_array == None:\n",
        "        return np.zeros(63)\n",
        "    elif marker == 'R' and np_array != None:\n",
        "        landmark = np_array.landmark\n",
        "\n",
        "    if marker == 'F' and np_array == None:\n",
        "        return np.zeros(1404)\n",
        "    elif marker == 'F' and np_array != None:\n",
        "        landmark = np_array.landmark\n",
        "\n",
        "    test_x = []\n",
        "    test_y = []\n",
        "    test_z = []\n",
        "    test = []\n",
        "    pose = []\n",
        "\n",
        "    if viz_flag:\n",
        "        test_visibility = []\n",
        "\n",
        "    for res in landmark:\n",
        "        if viz_flag:\n",
        "            test = np.array([res.x, res.y, res.z, res.visibility])\n",
        "        else:\n",
        "            test = np.array([res.x, res.y, res.z])\n",
        "\n",
        "        pose.append(test)\n",
        "        test_x.append(res.x)\n",
        "        test_y.append(res.y)\n",
        "        test_z.append(res.z)\n",
        "        if viz_flag:\n",
        "            test_visibility.append(res.visibility)\n",
        "\n",
        "    test_min_max_norm = []\n",
        "\n",
        "    min_test_x = min(test_x)\n",
        "    max_test_x = max(test_x)\n",
        "\n",
        "    min_test_y = min(test_y)\n",
        "    max_test_y = max(test_y)\n",
        "\n",
        "    min_test_z = min(test_z)\n",
        "    max_test_z = max(test_z)\n",
        "\n",
        "    if viz_flag:\n",
        "        min_test_visibility = min(test_visibility)\n",
        "        max_test_visibility = max(test_visibility)\n",
        "\n",
        "    for e in pose:\n",
        "        if viz_flag:\n",
        "             test_x, test_y, test_z, test_visibility = e[0], e[1], e[2], e[3]\n",
        "        else:\n",
        "             test_x, test_y, test_z = e[0], e[1], e[2]\n",
        "\n",
        "        test_x_norm = (test_x - min_test_x) / (max_test_x - min_test_x)\n",
        "        test_y_norm = (test_y - min_test_y) / (max_test_y - min_test_y)\n",
        "        test_z_norm = (test_z - min_test_z) / (max_test_z - min_test_z)\n",
        "        if viz_flag:\n",
        "            test_visibility_norm = (test_visibility - min_test_visibility) / (max_test_visibility - min_test_visibility)\n",
        "        if viz_flag:\n",
        "            test_min_max_norm.append(np.array([test_x_norm, test_y_norm, test_z_norm, test_visibility_norm]))\n",
        "        else:\n",
        "            test_min_max_norm.append(np.array([test_x_norm, test_y_norm, test_z_norm]))\n",
        "    return np.array(test_min_max_norm).flatten()"
      ],
      "metadata": {
        "id": "AHDCsmOq1x7m"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extract_min_max_norm_kp"
      ],
      "metadata": {
        "id": "m-zzBwaETM63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_min_max_norm_kp(results):\n",
        "    pose_min_max_norm = landmark_min_max_norm(results.pose_landmarks, True, 'P')\n",
        "    lh_min_max_norm = landmark_min_max_norm(results.left_hand_landmarks, False, 'L')\n",
        "    rh_min_max_norm = landmark_min_max_norm(results.right_hand_landmarks, False, 'R')\n",
        "    face_min_max_norm = landmark_min_max_norm(results.face_landmarks, False, 'F')\n",
        "    return np.concatenate([pose_min_max_norm, face_min_max_norm, lh_min_max_norm, rh_min_max_norm])"
      ],
      "metadata": {
        "id": "hvK_ASM611Yt"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## grayscale_resize_hist_equalize_image"
      ],
      "metadata": {
        "id": "MKV8AlqoTCne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grayscale_resize_hist_equalize_image(img, base_video_file_name, frame, image_size = IMAGE_SIZE):\n",
        "    \"\"\"\n",
        "    The method converts the image to grayscale, resizes and does histogram equalization.\n",
        "    \"\"\"\n",
        "    img = cv2.resize(img, (image_size, image_size))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img = exposure.equalize_hist(img)\n",
        "    #img = remove(Image.fromarray(img))\n",
        "    #plt.imshow(img, cmap = 'gray')\n",
        "    #plt.show(block = False)\n",
        "    file_name = base_video_file_name.split(\".mp4\")[0] + \"_f\" + str(frame).zfill(4) + \".jpeg\"\n",
        "    img_file_name = os.path.join(TEMP_DIR, file_name)\n",
        "    #img /= 255.0\n",
        "    #cv2.imwrite(img_file_name, 255 * img)\n",
        "    #result = cv2.normalize(img, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "    plt.imsave(img_file_name, img, cmap='gray')\n",
        "    return img"
      ],
      "metadata": {
        "id": "qFvIsNP3nnbc"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate_synthetic_video"
      ],
      "metadata": {
        "id": "PM7U14Dcpm7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_video(input_qualified_video_file, dir_path, label, counter):\n",
        "    \"\"\"\n",
        "    This method generates synthetic video data.\n",
        "    \"\"\"\n",
        "    # Set up image augmentation pipeline\n",
        "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "    seq = iaa.Sequential(\n",
        "              [\n",
        "               iaa.Crop(percent=(0, 0.1)), # random crops\n",
        "\n",
        "               # Small gaussian blur with random sigma between 0 and 0.5.\n",
        "               # But we only blur about 50% of all images.\n",
        "               iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))),\n",
        "\n",
        "               # Strengthen or weaken the contrast in each image.\n",
        "               iaa.LinearContrast((0.75, 1.5)),\n",
        "\n",
        "               # Add gaussian noise.\n",
        "               # For 50% of all images, we sample the noise once per pixel.\n",
        "               # For the other 50% of all images, we sample the noise per pixel AND\n",
        "               # channel. This can change the color (not only brightness) of the\n",
        "               # pixels.\n",
        "               iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
        "\n",
        "               # Make some images brighter and some darker.\n",
        "               # In 20% of all cases, we sample the multiplier once per channel,\n",
        "               # which can end up changing the color of the images.\n",
        "               iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
        "\n",
        "               # Apply affine transformations to each image.\n",
        "               # Scale/zoom them, translate/move them, rotate them and shear them.\n",
        "               sometimes(iaa.Affine(\n",
        "                                      translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "                                      shear=(-8, 8)\n",
        "                                  )\n",
        "                        ),\n",
        "               iaa.SomeOf(\n",
        "                             (0, 5),\n",
        "                             [iaa.OneOf([iaa.GaussianBlur((0, 3.0)),\n",
        "                                         iaa.AverageBlur(k=(2, 7)),\n",
        "                                         iaa.MedianBlur(k=(3, 11)),\n",
        "                                        ]\n",
        "                                       ),\n",
        "\n",
        "                              # Sharpen each image, overlay the result with the original\n",
        "                              # image using an alpha between 0 (no sharpening) and 1\n",
        "                              # (full sharpening effect).\n",
        "                              iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n",
        "\n",
        "                              # Same as sharpen, but for an embossing effect.\n",
        "                              iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
        "\n",
        "                              # Add gaussian noise to some images.\n",
        "                              # In 50% of these cases, the noise is randomly sampled per\n",
        "                              # channel and pixel.\n",
        "                              # In the other 50% of all cases it is sampled once per\n",
        "                              # pixel (i.e. brightness change).\n",
        "                              iaa.AdditiveGaussianNoise(loc=0,\n",
        "                                                        scale=(0.0, 0.05*255),\n",
        "                                                        per_channel=0.5\n",
        "                                                       ),\n",
        "\n",
        "                              # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
        "                              # them to black) or drop them on an image with 2-5% percent\n",
        "                              # of the original size, leading to large dropped\n",
        "                              # rectangles.\n",
        "                              iaa.OneOf([iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                                         iaa.CoarseDropout((0.03, 0.15),\n",
        "                                                           size_percent=(0.02, 0.05),\n",
        "                                                           per_channel=0.2\n",
        "                                                          ),\n",
        "                                        ]\n",
        "                                       ),\n",
        "\n",
        "                              # Add a value of -10 to 10 to each pixel.\n",
        "                              iaa.Add((-10, 10), per_channel=0.5),\n",
        "\n",
        "                              # Change brightness of images (50-150% of original value).\n",
        "                              iaa.Multiply((0.5, 1.5), per_channel=0.5),\n",
        "\n",
        "                              # Improve or worsen the contrast of images.\n",
        "                              iaa.LinearContrast((0.5, 2.0), per_channel=0.5)\n",
        "                             ]\n",
        "                         )\n",
        "\n",
        "              ],\n",
        "              random_order = True # apply augmenters in random order\n",
        "          )\n",
        "    # Input video file path\n",
        "    input_video_path = input_qualified_video_file\n",
        "    base_video_file_name = os.path.basename(input_qualified_video_file)\n",
        "\n",
        "    # Output video file path\n",
        "    output_video_file_name = base_video_file_name.split(\".mp4\")[0] + \"_syn_v\" + str(counter) + \".mp4\"\n",
        "    qualified_output_video_file_name = os.path.join(dir_path, label, output_video_file_name)\n",
        "\n",
        "    # Open the input video\n",
        "    video_reader = imageio.get_reader(input_video_path)\n",
        "\n",
        "    # Get video properties\n",
        "    fps = MAX_FRAME #reader.get_meta_data()['fps']\n",
        "    video_writer = imageio.get_writer(qualified_output_video_file_name, fps=fps)\n",
        "\n",
        "    # Iterate through each frame, apply augmentation, and save to the output video\n",
        "    for i, frame in enumerate(video_reader):\n",
        "        # Augment the frame\n",
        "        augmented_frame = seq.augment_image(frame)\n",
        "\n",
        "        # Append the augmented frame to the output video\n",
        "        video_writer.append_data(augmented_frame)\n",
        "\n",
        "    # Close the output video writer\n",
        "    video_writer.close()\n",
        "\n",
        "    #print(f\"Video augmentation completed. Augmented video saved to {qualified_output_video_file_name}\")\n"
      ],
      "metadata": {
        "id": "DHM01uL6pqdR"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate_synthetic_video_set"
      ],
      "metadata": {
        "id": "w2DUxIQWh69D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_video_set(iterations = 10):\n",
        "    \"\"\"\n",
        "    Generate synthetic video data.\n",
        "    \"\"\"\n",
        "    for label in label_list:\n",
        "        transformed_video_dir_path = os.path.join(TRANSFORMED_DATA_DIR, label)\n",
        "        for file in transformed_video_dir_path:\n",
        "            qualified_file_name = os.path.join(TRANSFORMED_DATA_DIR, label, file)\n",
        "            generate_synthetic_video(input_qualified_video_file = qualified_file_name,\n",
        "                                     dir_path = transformed_video_dir_path,\n",
        "                                     label = label,\n",
        "                                     counter = iterations\n",
        "                                    )\n",
        "\n",
        "        augmented_video_dir_path = os.path.join(AUGMENTED_DATA_DIR, label)\n",
        "        for file in augmented_video_dir_path:\n",
        "            qualified_file_name = os.path.join(augmented_video_dir_path, label, file)\n",
        "            generate_synthetic_video(input_qualified_video_file = qualified_file_name,\n",
        "                                     dir_path = augmented_video_dir_path,\n",
        "                                     label = label,\n",
        "                                     counter = iterations\n",
        "                                    )"
      ],
      "metadata": {
        "id": "gyARvHHrh5sG"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fast_or_slow"
      ],
      "metadata": {
        "id": "N7SOY-TG9d4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_or_slow(input_video_file_name, output_video_file_name, speed):\n",
        "    !ffmpeg -nostats -loglevel 0 -i {input_video_file_name} -vf  \"setpts={speed}*PTS\" -y {output_video_file_name}"
      ],
      "metadata": {
        "id": "tMPH4szJ9cc7"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_video"
      ],
      "metadata": {
        "id": "OKrfrFmvS-WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def create_video(dest_path, fps, label, base_video_file_name, source_path = TEMP_DIR, add_synthetic_video_flag = True, iterations = 2):\n",
        "    \"\"\"\n",
        "    Creates a video based on the given image list.\n",
        "    \"\"\"\n",
        "    #print(sorted(os.listdir(TEMP_DIR)))\n",
        "    transformed_video_file_name = os.path.join(dest_path, label, base_video_file_name)\n",
        "    #print(\"***********\")\n",
        "    #print(\"***********\")\n",
        "    #print(f\"Creating video file {transformed_video_file_name} for label : {label}\")\n",
        "    #print(f\"Souce files are located at {source_path}\")\n",
        "    #print(f\"Video file will be saved at {dest_path}\")\n",
        "    #print(f\"transformed_video_file_name : {transformed_video_file_name}, label : {label}\")\n",
        "    img_array = []\n",
        "    for filename in sorted(glob(f\"{source_path}/*.jpeg\")):\n",
        "        #print(f\"Processing {filename}\")\n",
        "        img = cv2.imread(filename)\n",
        "        #height, width, layers = img.shape\n",
        "        #size = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "        #print(img.shape)\n",
        "        img_array.append(img)\n",
        "\n",
        "    out = cv2.VideoWriter(transformed_video_file_name,\n",
        "                          cv2.VideoWriter_fourcc(*'MP4V'),\n",
        "                          fps,\n",
        "                          (IMAGE_SIZE, IMAGE_SIZE)\n",
        "                         )\n",
        "    for i in range(len(img_array)):\n",
        "        out.write(img_array[i])\n",
        "    out.release()\n",
        "    #print(f\"Video file status for {transformed_video_file_name}\")\n",
        "    #print(get_video_file_stats(transformed_video_file_name))\n",
        "\n",
        "    if add_synthetic_video_flag == True:\n",
        "        # Generate more synthetic data for the augmented video\n",
        "        for counter in range(iterations):\n",
        "            generate_synthetic_video(input_qualified_video_file = transformed_video_file_name,\n",
        "                                     dir_path = dest_path,\n",
        "                                     label = label,\n",
        "                                     counter = counter\n",
        "                                    )\n",
        "\n",
        "    #print(\"***********\")\n",
        "    #print(\"***********\")\n",
        "'''"
      ],
      "metadata": {
        "id": "zl_I5_wlAYdL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "6027a100-2077-4a56-f4bd-bf1ba9f82c36"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef create_video(dest_path, fps, label, base_video_file_name, source_path = TEMP_DIR, add_synthetic_video_flag = True, iterations = 2):\\n    \"\"\"\\n    Creates a video based on the given image list.\\n    \"\"\"\\n    #print(sorted(os.listdir(TEMP_DIR)))\\n    transformed_video_file_name = os.path.join(dest_path, label, base_video_file_name)\\n    #print(\"***********\")\\n    #print(\"***********\")\\n    #print(f\"Creating video file {transformed_video_file_name} for label : {label}\")\\n    #print(f\"Souce files are located at {source_path}\")\\n    #print(f\"Video file will be saved at {dest_path}\")\\n    #print(f\"transformed_video_file_name : {transformed_video_file_name}, label : {label}\")\\n    img_array = []\\n    for filename in sorted(glob(f\"{source_path}/*.jpeg\")):\\n        #print(f\"Processing {filename}\")\\n        img = cv2.imread(filename)\\n        #height, width, layers = img.shape\\n        #size = (IMAGE_SIZE, IMAGE_SIZE)\\n        #print(img.shape)\\n        img_array.append(img)\\n\\n    out = cv2.VideoWriter(transformed_video_file_name,\\n                          cv2.VideoWriter_fourcc(*\\'MP4V\\'),\\n                          fps,\\n                          (IMAGE_SIZE, IMAGE_SIZE)\\n                         )\\n    for i in range(len(img_array)):\\n        out.write(img_array[i])\\n    out.release()\\n    #print(f\"Video file status for {transformed_video_file_name}\")\\n    #print(get_video_file_stats(transformed_video_file_name))\\n\\n    if add_synthetic_video_flag == True:\\n        # Generate more synthetic data for the augmented video\\n        for counter in range(iterations):\\n            generate_synthetic_video(input_qualified_video_file = transformed_video_file_name,\\n                                     dir_path = dest_path,\\n                                     label = label,\\n                                     counter = counter\\n                                    )\\n\\n    #print(\"***********\")\\n    #print(\"***********\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_video(src_dir_path, tgt_dir_path, fps, height, width, tgt_video_file_name):\n",
        "    \"\"\"\n",
        "    Creates a video based on the given image list.\n",
        "    \"\"\"\n",
        "    #print(sorted(os.listdir(TEMP_DIR)))\n",
        "    img_array = []\n",
        "    for filename in sorted(glob(f\"{src_dir_path}/*.jpeg\")):\n",
        "        img = cv2.imread(filename)\n",
        "        img_array.append(img)\n",
        "    '''\n",
        "    out = cv2.VideoWriter(tgt_video_file_name,\n",
        "                          cv2.VideoWriter_fourcc(*'MP4V'),\n",
        "                          fps,\n",
        "                          (height, width)\n",
        "                         )\n",
        "    for i in range(len(img_array)):\n",
        "        out.write(img_array[i])\n",
        "    out.release()\n",
        "    '''\n",
        "    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(img_array, fps=fps)\n",
        "    clip.write_videofile(tgt_video_file_name)\n",
        "    #print(f\"Video file status for {tgt_video_file_name}\")\n",
        "    #print(get_video_file_stats(tgt_video_file_name))"
      ],
      "metadata": {
        "id": "m-qNq1Az914z"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocess_video_data"
      ],
      "metadata": {
        "id": "diJWyGtiUX4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_video_data(qualified_video_file_name, base_video_file_name, dest_path, label):\n",
        "    \"\"\"\n",
        "    This method reads the video file frame by frame. It performs preprocessing\n",
        "    on each image by resizing, grayscaling and performing histogram equalization.\n",
        "    It then creates a video based on max_frame size.\n",
        "    If the frame count is equal to the max frame count, the preprocessed image\n",
        "    is appended to a list.\n",
        "    If the frame count is less than max frame count, it will be padded with black\n",
        "    images at the end.\n",
        "    If the frame count is more than max frame count, it will chop off beyond\n",
        "    the max frame count.\n",
        "    \"\"\"\n",
        "    create_dir(TEMP_DIR)\n",
        "    !rm -rf {TEMP_DIR}/*.jpeg\n",
        "    cap = cv2.VideoCapture(qualified_video_file_name)\n",
        "    frame_count, height, width, fps = get_video_file_stats(qualified_video_file_name)\n",
        "    image_file_list = []\n",
        "    #print(\"\")\n",
        "    #print(\"\")\n",
        "    #print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
        "    #print(f\"Frame count for the given video : {frame_count}, Max frame : {MAX_FRAME}\")\n",
        "    if MAX_FRAME == frame_count:\n",
        "        #print(\"max_frame == frame_count\")\n",
        "        for frame in range(MAX_FRAME):\n",
        "            ret, image = cap.read()\n",
        "            if image is None:\n",
        "                break\n",
        "            image = grayscale_resize_hist_equalize_image(image, base_video_file_name, frame)\n",
        "            image_file_list.append(image)\n",
        "\n",
        "    elif MAX_FRAME < frame_count:\n",
        "\n",
        "        #print(\"max_frame < frame_count\")\n",
        "        for frame in range(MAX_FRAME):\n",
        "            ret, image = cap.read()\n",
        "            if image is None:\n",
        "                break\n",
        "            image = grayscale_resize_hist_equalize_image(image, base_video_file_name, frame)\n",
        "            image_file_list.append(image)\n",
        "\n",
        "    elif MAX_FRAME > frame_count:\n",
        "\n",
        "        #print(\"max_frame > frame_count\")\n",
        "        for frame in range(MAX_FRAME):\n",
        "            ret, image = cap.read()\n",
        "            if image is None or frame >= frame_count:\n",
        "                image = 255 * np.ones((IMAGE_SIZE, IMAGE_SIZE), dtype = np.uint8) #np.zeros_like(prev_img)\n",
        "                file_name = base_video_file_name.split(\".mp4\")[0] + \"_f\" + str(frame).zfill(4) + \".jpeg\"\n",
        "                img_file_name = os.path.join(TEMP_DIR, file_name)\n",
        "                plt.imsave(img_file_name, image, cmap='gray')\n",
        "                image_file_list.append(image)\n",
        "            else:\n",
        "                image = grayscale_resize_hist_equalize_image(image, base_video_file_name, frame)\n",
        "                image_file_list.append(image)\n",
        "\n",
        "    cap.release()\n",
        "    create_video(dest_path = dest_path,\n",
        "                 fps = FPS,\n",
        "                 label = label,\n",
        "                 base_video_file_name = base_video_file_name,\n",
        "                 source_path = TEMP_DIR,\n",
        "                 add_synthetic_video_flag = True,\n",
        "                 iterations = 2\n",
        "                )\n",
        "    #!rm -rf {TEMP_DIR}/*.jpeg\n",
        "    #print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
        "    #print(\"\")\n",
        "    #print(\"\")"
      ],
      "metadata": {
        "id": "4MKEnR8IBylc"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocess_video_data_set"
      ],
      "metadata": {
        "id": "1RGysBNEYVwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_video_data_set(root_dir = ROOT_DIR, sign_language_category = \"asl\"):\n",
        "    \"\"\"\n",
        "    This method preprocesses the video data set present in video folders.\n",
        "    \"\"\"\n",
        "    dir_path = os.path.join(root_dir, f\"{sign_language_category}_dataset\", \"stage\")\n",
        "    label_list = sorted(os.listdir(os.path.join(ROOT_DIR, f\"{sign_language_category}_dataset\", \"stage\")))\n",
        "    for label in label_list:\n",
        "        label_video_path = os.path.join(dir_path, label, \"video\")\n",
        "        #print(label_video_path)\n",
        "\n",
        "        label_video_list = os.listdir(label_video_path)\n",
        "        for label_video in label_video_list:\n",
        "            video_file_name = os.path.join(label_video_path, label_video)\n",
        "            #print(f\"video_file_name : {video_file_name}\")\n",
        "            base_video_file_name = os.path.basename(video_file_name)\n",
        "            preprocess_video_data(qualified_video_file_name = video_file_name,\n",
        "                                  base_video_file_name = base_video_file_name,\n",
        "                                  dest_path = TRANSFORMED_DATA_DIR,\n",
        "                                  label = label)"
      ],
      "metadata": {
        "id": "BURHpE66YYiZ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_images_in_dir"
      ],
      "metadata": {
        "id": "c2L2FmRn476a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images_in_dir(image_list, image_file_name_list, dir_path):\n",
        "    \"\"\"\n",
        "    This method saves given image list in a path.\n",
        "    \"\"\"\n",
        "    for image, image_name in zip(image_list, image_file_name_list):\n",
        "        qualified_img_file_name = os.path.join(dir_path, image_name)\n",
        "        plt.imsave(qualified_img_file_name, image, cmap='gray')"
      ],
      "metadata": {
        "id": "S0nLdG7-4_-s"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## video_augmentation"
      ],
      "metadata": {
        "id": "XDB4NgRu7x3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def video_augmentation(qualified_video_file_name, label, target_path):\n",
        "    \"\"\"\n",
        "    This method augments a video.\n",
        "    \"\"\"\n",
        "    create_dir(TEMP_DIR)\n",
        "    !rm -rf {TEMP_DIR}/*.jpeg\n",
        "    cap = cv2.VideoCapture(qualified_video_file_name)\n",
        "    frame_count, height, width, fps = get_video_file_stats(qualified_video_file_name)\n",
        "    image_file_list = []\n",
        "    image_file_name_list = []\n",
        "    aug_image_file_name_list = []\n",
        "\n",
        "    \"\"\"\n",
        "    Read the video file frame by frame and save it in TEMP_DIR.\n",
        "    \"\"\"\n",
        "    for frame in range(frame_count):\n",
        "        ret, image = cap.read()\n",
        "        if image is None:\n",
        "            break\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        image_file_list.append(image)\n",
        "        image_file_name = os.path.basename(qualified_video_file_name).split(\".mp4\")[0] + \"_f\" + str(frame).zfill(4) + \".jpeg\"\n",
        "        qualified_image_file_name = os.path.join(TEMP_DIR, image_file_name)\n",
        "        image_file_name_list.append(image_file_name)\n",
        "        plt.imsave(qualified_image_file_name, image)\n",
        "    cap.release()\n",
        "\n",
        "    \"\"\"\n",
        "    Perform video augmentation as defined in AUGMENT_FUNCTION_SET.\n",
        "    As the video is stack of images, the way we do video augmentation is to\n",
        "    augment each image and create a video off the augmented images.\n",
        "    \"\"\"\n",
        "    for augment_action in AUGMENT_FUNCTION_SET:\n",
        "        print(f\"Augmentation function: {augment_action}\")\n",
        "        temp_image_file_list = []\n",
        "        if augment_action == 'blur':\n",
        "            temp_image_file_list = blur_images(image_file_list, KERNEL_SIZE)\n",
        "\n",
        "        elif augment_action == 'sharpen':\n",
        "            temp_image_file_list = sharpen_images(image_file_list)\n",
        "\n",
        "        elif augment_action == 'adjust_contrast_brightness':\n",
        "            temp_image_file_list = adjust_contrast_brightness(image_file_list)\n",
        "\n",
        "        elif augment_action == 'adjust_gamma':\n",
        "            temp_image_file_list = adjust_gamma(image_file_list)\n",
        "\n",
        "        elif augment_action == 'dilate':\n",
        "            temp_image_file_list = dilate_images(image_file_list, KERNEL_SIZE, ITERATIONS)\n",
        "\n",
        "        elif augment_action == 'erode':\n",
        "            temp_image_file_list = erode_images(image_file_list, KERNEL_SIZE, ITERATIONS)\n",
        "\n",
        "        elif augment_action == 'flip_h':\n",
        "            temp_image_file_list = flip_images(image_file_list, 1)\n",
        "\n",
        "        elif augment_action == 'flip_v':\n",
        "            temp_image_file_list = flip_images(image_file_list, 0)\n",
        "\n",
        "        elif augment_action == 'apply_log_transform':\n",
        "            temp_image_file_list = apply_log_transform_imagesets(image_file_list)\n",
        "\n",
        "        elif augment_action == 'rotate_image_by_angle':\n",
        "            temp_image_file_list = rotate_image_by_angle(image_file_list, random.randint(-45, 45))\n",
        "\n",
        "        elif augment_action == 'canny':\n",
        "            temp_image_file_list = get_canny_feature(image_file_list)\n",
        "\n",
        "        elif augment_action == 'sobel':\n",
        "            temp_image_file_list = get_sobel_feature(image_file_list)\n",
        "\n",
        "        elif augment_action == 'prewitth':\n",
        "            temp_image_file_list = get_prewitth_kernel_feature(image_file_list)\n",
        "\n",
        "        elif augment_action == 'prewittv':\n",
        "            temp_image_file_list = get_prewittv_kernel_feature(image_file_list)\n",
        "\n",
        "        elif augment_action == 'contour':\n",
        "            temp_image_file_list = get_contour_feature(image_file_list)\n",
        "\n",
        "        elif augment_action == 'rotate_90_clkwise':\n",
        "            temp_image_file_list = rotate_images(image_file_list, 1)\n",
        "\n",
        "        elif augment_action == 'rotate_180':\n",
        "            temp_image_file_list = rotate_images(image_file_list, 2)\n",
        "\n",
        "        elif augment_action == 'add_periodic_noise':\n",
        "            temp_image_file_list = add_periodic_noise(image_file_list)\n",
        "\n",
        "        aug_base_video_file_name = os.path.basename(qualified_video_file_name).split(\".mp4\")[0] + f'_{augment_action}.mp4'\n",
        "        #print(f\"Augmented video file name : {aug_base_video_file_name}\")\n",
        "        aug_image_file_name_list = [image.split(\".jpeg\")[0] + f\"_{augment_action}.jpeg\" for image in image_file_name_list]\n",
        "        dir_path = f\"{ROOT_DIR}/temp_{label}_{augment_action}\"\n",
        "        remove_dir(dir_path)\n",
        "        create_dir(dir_path)\n",
        "        save_images_in_dir(image_list = temp_image_file_list,\n",
        "                           image_file_name_list = aug_image_file_name_list,\n",
        "                           dir_path = dir_path)\n",
        "        create_video(dest_path = target_path,\n",
        "                     fps = FPS,\n",
        "                     label = label,\n",
        "                     base_video_file_name = aug_base_video_file_name,\n",
        "                     source_path = dir_path,\n",
        "                     add_synthetic_video_flag = False,\n",
        "                     iterations = 0\n",
        "                    )\n",
        "        #remove_dir(dir_path)\n",
        "'''"
      ],
      "metadata": {
        "id": "ixdzeYMS2f_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "de552c9e-abdc-47a5-f47d-c2ac1c014597"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef video_augmentation(qualified_video_file_name, label, target_path):\\n    \"\"\"\\n    This method augments a video.\\n    \"\"\"\\n    create_dir(TEMP_DIR)\\n    !rm -rf {TEMP_DIR}/*.jpeg\\n    cap = cv2.VideoCapture(qualified_video_file_name)\\n    frame_count, height, width, fps = get_video_file_stats(qualified_video_file_name)\\n    image_file_list = []\\n    image_file_name_list = []\\n    aug_image_file_name_list = []\\n\\n    \"\"\"\\n    Read the video file frame by frame and save it in TEMP_DIR.\\n    \"\"\"\\n    for frame in range(frame_count):\\n        ret, image = cap.read()\\n        if image is None:\\n            break\\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n        image_file_list.append(image)\\n        image_file_name = os.path.basename(qualified_video_file_name).split(\".mp4\")[0] + \"_f\" + str(frame).zfill(4) + \".jpeg\"\\n        qualified_image_file_name = os.path.join(TEMP_DIR, image_file_name)\\n        image_file_name_list.append(image_file_name)\\n        plt.imsave(qualified_image_file_name, image)\\n    cap.release()\\n\\n    \"\"\"\\n    Perform video augmentation as defined in AUGMENT_FUNCTION_SET.\\n    As the video is stack of images, the way we do video augmentation is to\\n    augment each image and create a video off the augmented images.\\n    \"\"\"\\n    for augment_action in AUGMENT_FUNCTION_SET:\\n        print(f\"Augmentation function: {augment_action}\")\\n        temp_image_file_list = []\\n        if augment_action == \\'blur\\':\\n            temp_image_file_list = blur_images(image_file_list, KERNEL_SIZE)\\n\\n        elif augment_action == \\'sharpen\\':\\n            temp_image_file_list = sharpen_images(image_file_list)\\n\\n        elif augment_action == \\'adjust_contrast_brightness\\':\\n            temp_image_file_list = adjust_contrast_brightness(image_file_list)\\n\\n        elif augment_action == \\'adjust_gamma\\':\\n            temp_image_file_list = adjust_gamma(image_file_list)\\n\\n        elif augment_action == \\'dilate\\':\\n            temp_image_file_list = dilate_images(image_file_list, KERNEL_SIZE, ITERATIONS)\\n\\n        elif augment_action == \\'erode\\':\\n            temp_image_file_list = erode_images(image_file_list, KERNEL_SIZE, ITERATIONS)\\n\\n        elif augment_action == \\'flip_h\\':\\n            temp_image_file_list = flip_images(image_file_list, 1)\\n\\n        elif augment_action == \\'flip_v\\':\\n            temp_image_file_list = flip_images(image_file_list, 0)\\n\\n        elif augment_action == \\'apply_log_transform\\':\\n            temp_image_file_list = apply_log_transform_imagesets(image_file_list)\\n\\n        elif augment_action == \\'rotate_image_by_angle\\':\\n            temp_image_file_list = rotate_image_by_angle(image_file_list, random.randint(-45, 45))\\n\\n        elif augment_action == \\'canny\\':\\n            temp_image_file_list = get_canny_feature(image_file_list)\\n\\n        elif augment_action == \\'sobel\\':\\n            temp_image_file_list = get_sobel_feature(image_file_list)\\n\\n        elif augment_action == \\'prewitth\\':\\n            temp_image_file_list = get_prewitth_kernel_feature(image_file_list)\\n\\n        elif augment_action == \\'prewittv\\':\\n            temp_image_file_list = get_prewittv_kernel_feature(image_file_list)\\n\\n        elif augment_action == \\'contour\\':\\n            temp_image_file_list = get_contour_feature(image_file_list)\\n\\n        elif augment_action == \\'rotate_90_clkwise\\':\\n            temp_image_file_list = rotate_images(image_file_list, 1)\\n\\n        elif augment_action == \\'rotate_180\\':\\n            temp_image_file_list = rotate_images(image_file_list, 2)\\n\\n        elif augment_action == \\'add_periodic_noise\\':\\n            temp_image_file_list = add_periodic_noise(image_file_list)\\n\\n        aug_base_video_file_name = os.path.basename(qualified_video_file_name).split(\".mp4\")[0] + f\\'_{augment_action}.mp4\\'\\n        #print(f\"Augmented video file name : {aug_base_video_file_name}\")\\n        aug_image_file_name_list = [image.split(\".jpeg\")[0] + f\"_{augment_action}.jpeg\" for image in image_file_name_list]\\n        dir_path = f\"{ROOT_DIR}/temp_{label}_{augment_action}\"\\n        remove_dir(dir_path)\\n        create_dir(dir_path)\\n        save_images_in_dir(image_list = temp_image_file_list,\\n                           image_file_name_list = aug_image_file_name_list,\\n                           dir_path = dir_path)\\n        create_video(dest_path = target_path,\\n                     fps = FPS,\\n                     label = label,\\n                     base_video_file_name = aug_base_video_file_name,\\n                     source_path = dir_path,\\n                     add_synthetic_video_flag = False,\\n                     iterations = 0\\n                    )\\n        #remove_dir(dir_path)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def video_augmentation(src_dir_path, tgt_dir_path):\n",
        "    src_video_file_list = sorted(glob(f'{src_dir_path}/*.mp4'))\n",
        "    for src_video_file in src_video_file_list:\n",
        "\n",
        "        remove_temp_dir()\n",
        "        create_dir(TEMP_DIR)\n",
        "        create_dir(TEMP_AUG_DIR)\n",
        "        qualified_video_file_name = src_video_file\n",
        "        src_video_file_base_name = os.path.basename(qualified_video_file_name)\n",
        "        output_fast_video_file_name = src_video_file_base_name.split(\".mp4\")[0] + \"_fast.mp4\"\n",
        "        output_slow_video_file_name = src_video_file_base_name.split(\".mp4\")[0] + \"_slow.mp4\"\n",
        "        qualified_output_fast_video_file_name = os.path.join(tgt_dir_path, output_fast_video_file_name)\n",
        "        qualified_output_slow_video_file_name = os.path.join(tgt_dir_path, output_slow_video_file_name)\n",
        "        #print(qualified_output_fast_video_file_name, qualified_output_slow_video_file_name)\n",
        "\n",
        "        fast_or_slow(input_video_file_name = qualified_video_file_name,\n",
        "                     output_video_file_name = qualified_output_fast_video_file_name,\n",
        "                     speed = FAST_SPPED\n",
        "                    )\n",
        "        fast_or_slow(input_video_file_name = qualified_video_file_name,\n",
        "                     output_video_file_name = qualified_output_slow_video_file_name,\n",
        "                     speed = SLOW_SPEED\n",
        "                    )\n",
        "\n",
        "        cap = cv2.VideoCapture(qualified_video_file_name)\n",
        "        frame_count, height, width, fps = get_video_file_stats(qualified_video_file_name)\n",
        "        #print(frame_count, height, width, fps)\n",
        "        image_file_list = []\n",
        "        image_file_name_list = []\n",
        "        for frame in range(frame_count):\n",
        "            ret, image = cap.read()\n",
        "            if image is None:\n",
        "                break\n",
        "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            image_file_list.append(image)\n",
        "            image_file_name = os.path.basename(qualified_video_file_name).split(\".mp4\")[0] + \"_f\" + str(frame).zfill(4) + \".jpeg\"\n",
        "            image_file_name_list.append(image_file_name)\n",
        "            qualified_image_file_name = os.path.join(TEMP_DIR, image_file_name)\n",
        "            plt.imsave(qualified_image_file_name, image)\n",
        "        cap.release()\n",
        "        image_file_list = resize_images(images = image_file_list, image_size = IMAGE_SIZE)\n",
        "        for augment_action in AUGMENT_FUNCTION_LIST:\n",
        "            print(\"**************************************************\")\n",
        "            print(f\"Augmentation function: {augment_action}\")\n",
        "            if augment_action == 'make_fast' or augment_action == 'make_slow':\n",
        "                continue\n",
        "            temp_image_file_list = []\n",
        "            if augment_action == 'blur':\n",
        "                temp_image_file_list = blur_images(image_file_list, KERNEL_SIZE)\n",
        "\n",
        "            elif augment_action == 'sharpen':\n",
        "                temp_image_file_list = sharpen_images(image_file_list)\n",
        "\n",
        "            elif augment_action == 'adjust_contrast_brightness':\n",
        "                temp_image_file_list = adjust_contrast_brightness(image_file_list)\n",
        "\n",
        "            elif augment_action == 'adjust_gamma':\n",
        "                temp_image_file_list = adjust_gamma(image_file_list)\n",
        "\n",
        "            elif augment_action == 'dilate':\n",
        "                temp_image_file_list = dilate_images(image_file_list, KERNEL_SIZE, ITERATIONS)\n",
        "\n",
        "            elif augment_action == 'erode':\n",
        "                temp_image_file_list = erode_images(image_file_list, KERNEL_SIZE, ITERATIONS)\n",
        "\n",
        "            elif augment_action == 'flip':\n",
        "                temp_image_file_list = flip_images(image_file_list, random.choice(range(0, 2)))\n",
        "\n",
        "            elif augment_action == 'rotate_90_or_180':\n",
        "                temp_image_file_list = rotate_images(image_file_list, random.choice(range(1, 3)))\n",
        "\n",
        "            elif augment_action == 'add_noise':\n",
        "                None\n",
        "                '''\n",
        "                noise_type_list = ['gaussian', 'localvar', 'poisson', 'salt', 'pepper', 's&p', 'speckle']\n",
        "                noise_type = noise_type_list[random.choice(range(0, len(noise_type_list)))]\n",
        "                temp_image_file_list = add_noise(image_file_list, noise_type)\n",
        "                '''\n",
        "\n",
        "            elif augment_action == 'rotate_by_angle':\n",
        "                temp_image_file_list = rotate_image_by_angle(image_file_list, random.choice(range(-45, 46)))\n",
        "\n",
        "            aug_base_video_file_name = os.path.basename(qualified_video_file_name).split(\".mp4\")[0] + f'_{augment_action}.mp4'\n",
        "            aug_image_file_name_list = [image.split(\".jpeg\")[0] + f\"_{augment_action}.jpeg\" for image in image_file_name_list]\n",
        "            save_images_in_dir(image_list = temp_image_file_list,\n",
        "                               image_file_name_list = aug_image_file_name_list,\n",
        "                               dir_path = TEMP_AUG_DIR\n",
        "                              )\n",
        "            tgt_video_file_name = os.path.join(tgt_dir_path, aug_base_video_file_name)\n",
        "            create_video(src_dir_path = TEMP_AUG_DIR,\n",
        "                         tgt_dir_path = tgt_dir_path,\n",
        "                         fps = fps,\n",
        "                         height = height,\n",
        "                         width = width,\n",
        "                         tgt_video_file_name = tgt_video_file_name\n",
        "                        )"
      ],
      "metadata": {
        "id": "g2JHcZtW-AI0"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## process_video_augmentation"
      ],
      "metadata": {
        "id": "aXMTspyP8eel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_augmentation(sign_language_category = 'asl'):\n",
        "    \"\"\"\n",
        "    This method does the augmentation on the video data set.\n",
        "    \"\"\"\n",
        "    dir_path = os.path.join(ROOT_DIR, f\"{sign_language_category}_dataset\", \"transformed_data\")\n",
        "    label_list = sorted(os.listdir(dir_path))\n",
        "    for label in label_list:\n",
        "        label_video_path = os.path.join(dir_path, label)\n",
        "        video_files_list = sorted(os.listdir(label_video_path))\n",
        "        for video in video_files_list:\n",
        "            qualified_video_file_name = os.path.join(label_video_path, video)\n",
        "            #print(f\"Augmenting file : {qualified_video_file_name}\")\n",
        "            target_path = AUGMENTED_DATA_DIR\n",
        "            video_augmentation(qualified_video_file_name, label, target_path)"
      ],
      "metadata": {
        "id": "dFJdr5Cl8hkh"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_file_category"
      ],
      "metadata": {
        "id": "Y2Nzgaz2cwZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_category(source_path):\n",
        "    \"\"\"\n",
        "    This method fetches file category based on the source path.\n",
        "    The file category is used to fetch number of files under each label.\n",
        "    \"\"\"\n",
        "    file_category = \"\"\n",
        "    if source_path == TRANSFORMED_DATA_DIR:\n",
        "        file_category = 'transformed'\n",
        "    elif source_path == AUGMENTED_DATA_DIR:\n",
        "        file_category = 'augmented'\n",
        "    elif source_path == ORIG_MEDIAPIPE_DATA_DIR:\n",
        "        file_category = 'orig_mediapipe'\n",
        "    elif source_path == AUG_MEDIAPIPE_DATA_DIR:\n",
        "        file_category = 'aug_mediapipe'\n",
        "    elif source_path == CONSOLIDATED_MEDIAPIPE_DATA_DIR:\n",
        "        file_category = 'consolidated_mediapipe'\n",
        "    elif source_path == TRANSFORMED_IMAGE_DIR:\n",
        "        file_category = 'transformed_images'\n",
        "    elif source_path == TRANSFORMED_AUGMENTED_IMAGE_DIR:\n",
        "        file_category = 'transformed_augmented_images'\n",
        "    return file_category"
      ],
      "metadata": {
        "id": "24LAr83lczNv"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## build_file_list"
      ],
      "metadata": {
        "id": "XpJXtFh_Ilc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_file_list(source_path, extension):\n",
        "    \"\"\"\n",
        "    This method builds dataframe to store file names, along with label.\n",
        "    \"\"\"\n",
        "    file_list = []\n",
        "    global df_file_list\n",
        "    file_category = get_file_category(source_path = source_path)\n",
        "    for label in label_list:\n",
        "        label_id = label_map[label]\n",
        "        for file in os.listdir(os.path.join(source_path, label)):\n",
        "            file_list.append((file_category, label, label_id, file, os.path.join(source_path, label, file)))\n",
        "    temp_df = pd.DataFrame(file_list)\n",
        "    temp_df.columns = ['file_category', 'label', 'label_id', 'file_name', 'qualified_file_name']\n",
        "    temp_df['base_file_name'] = temp_df['file_name'].apply(lambda x : x.split(f\".{extension}\")[0])\n",
        "    #temp_df['media_pipe_dir_path'] = ''\n",
        "    temp_df.columns = ['file_category', 'label', 'label_id', 'file_name', 'qualified_file_name', 'base_file_name'] #'media_pipe_dir_path'\n",
        "    df_file_list = pd.concat([df_file_list, temp_df])\n",
        "    df_file_list.columns = ['file_category', 'label', 'label_id', 'file_name', 'qualified_file_name', 'base_file_name']#'media_pipe_dir_path'\n",
        "    '''\n",
        "    df_file_list['media_pipe_dir_path'] = np.where((df_file_list.file_category == 'transformed'),\n",
        "                                                    df_file_list['qualified_file_name'].apply(lambda x : x.replace(\"transformed_data\",\"orig_mediapipe_data\").split(\".mp4\")[0]),\n",
        "                                                    df_file_list['qualified_file_name'].apply(lambda x : x.replace(\"augmented_data\",\"aug_mediapipe_data\").split(\".mp4\")[0])\n",
        "                                                  )\n",
        "    '''"
      ],
      "metadata": {
        "id": "BaYC_s7uIj1U"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## consolidate_input_files"
      ],
      "metadata": {
        "id": "gBRMUDpQAETw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_input_files():\n",
        "    \"\"\"\n",
        "    This method consolidates input files and moves them under consolidated_data folder.\n",
        "    \"\"\"\n",
        "    for label in label_list:\n",
        "        transformed_video_path = os.path.join(TRANSFORMED_DATA_DIR, label)\n",
        "        augmented_video_path = os.path.join(AUGMENTED_DATA_DIR, label)\n",
        "        transformed_video_list = [os.path.join(transformed_video_path, file) for file in os.listdir(transformed_video_path)]\n",
        "        augmented_video_list = [os.path.join(augmented_video_path, file) for file in os.listdir(augmented_video_path)]\n",
        "        consolidated_video_list = transformed_video_list + augmented_video_list\n",
        "        target_path = os.path.join(CONSOLIDATED_MEDIAPIPE_DATA_DIR, label)\n",
        "        print(f\"Processing label {label}, moving {len(consolidated_video_list)} files to the path {target_path}\")\n",
        "        process_copy_files(file_name_list = consolidated_video_list, dest_path = target_path)"
      ],
      "metadata": {
        "id": "UVEmEefOAHjX"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## show_file_count_per_label"
      ],
      "metadata": {
        "id": "Uxqen22VS0ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_file_count_per_label(source_path, sign_language_category = 'asl'):\n",
        "    \"\"\"\n",
        "    This method generates file count per label\n",
        "    \"\"\"\n",
        "    file_category = get_file_category(source_path = source_path)\n",
        "\n",
        "    for label in label_list:\n",
        "        file_list = [os.path.join(source_path, label, file)\n",
        "                     for file in os.listdir(os.path.join(source_path, label))\n",
        "                    ]\n",
        "\n",
        "        file_count_per_label_dict[sign_language_category][file_category][label] = [file_list, len(file_list)]\n",
        "        print(f\"Processing label {label}, file count : {len(file_list)}\")"
      ],
      "metadata": {
        "id": "IXW6pfoNS6VZ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_mediapipe_dir_struct"
      ],
      "metadata": {
        "id": "POpUSY1TdVND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mediapipe_dir_struct(source_path = TRANSFORMED_DATA_DIR, target_path = ORIG_MEDIAPIPE_DATA_DIR, sign_language_category = 'asl'):\n",
        "    \"\"\"\n",
        "    This method creates mediapipe directory structure for each label input file\n",
        "    under MEDIAPIPE_DATA_DIR.\n",
        "    \"\"\"\n",
        "    file_category = get_file_category(source_path = source_path)\n",
        "\n",
        "    for label in label_list:\n",
        "        number_of_input_files = file_count_per_label_dict[sign_language_category][file_category][label][1]\n",
        "        file_list = file_count_per_label_dict[sign_language_category][file_category][label][0]\n",
        "        for file in file_list:\n",
        "            base_video_file_name_wo_extn = os.path.basename(file).split(\".mp4\")[0]\n",
        "            dir_name = os.path.join(target_path, label, base_video_file_name_wo_extn)\n",
        "            create_dir(dir_name)"
      ],
      "metadata": {
        "id": "tqQAp9wWdW0W"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## collect_kp_from_video_and_save"
      ],
      "metadata": {
        "id": "N8gwG2wqTLSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_and_save_kp_data_for_video(source_path, dest_path, label, sign_language_category = 'asl', frame_count = MAX_FRAME):\n",
        "    \"\"\"\n",
        "    This method reads videob data frame by frame, collects KP data and\n",
        "    saves them under respective medipipe folder.\n",
        "    \"\"\"\n",
        "    file_category = get_file_category(source_path = source_path)\n",
        "\n",
        "    number_of_label_videos = file_count_per_label_dict[sign_language_category][file_category][label][1]\n",
        "    video_list = file_count_per_label_dict[sign_language_category][file_category][label][0]\n",
        "\n",
        "    # Set mediapipe model\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        # Loop through sequences aka videos\n",
        "        for file_idx in range(1, number_of_label_videos + 1):\n",
        "            video_name = video_list[file_idx - 1]\n",
        "            cap = cv2.VideoCapture(video_name)\n",
        "            frame_count, height, width, fps = get_video_file_stats(video_file_name = video_name)\n",
        "            # Loop through video length aka sequence length\n",
        "            for frame_num in range(1, frame_count + 1):\n",
        "                #print(f\"Processing video name : {video_name}, frame : {frame_num}\")\n",
        "                # Read feed\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                if frame is None:\n",
        "                    break\n",
        "\n",
        "                # Make detections\n",
        "                image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "                # Draw landmarks\n",
        "                # draw_styled_landmarks(image, results)\n",
        "\n",
        "                '''\n",
        "                # NEW Apply wait logic\n",
        "                if frame_num == 0:\n",
        "                    cv2.putText(image,\n",
        "                                'STARTING COLLECTION',\n",
        "                                (120,200),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                                1,\n",
        "                                (0,255, 0),\n",
        "                                4,\n",
        "                                cv2.LINE_AA\n",
        "                               )\n",
        "                    cv2.putText(image,\n",
        "                                f'Collecting KP for label: {label}, video: {video_name}, frame: {frame_num}',\n",
        "                                (15,12),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                                0.5,\n",
        "                                (0, 0, 255),\n",
        "                                1,\n",
        "                                cv2.LINE_AA\n",
        "                               )\n",
        "                    # Show to screen\n",
        "                    cv2_imshow(image)\n",
        "                    cv2.waitKey(500)\n",
        "                else:\n",
        "                    cv2.putText(image,\n",
        "                                f'Collecting KP for label: {label}, video: {video_name}, frame: {frame_num}',\n",
        "                                (15,12),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                                0.5,\n",
        "                                (0, 0, 255),\n",
        "                                1,\n",
        "                                cv2.LINE_AA\n",
        "                               )\n",
        "                    # Show to screen\n",
        "                    cv2_imshow(image)\n",
        "                '''\n",
        "                # NEW Export keypoints\n",
        "                keypoints = extract_keypoints(results)\n",
        "                keypoints_min_max_norm = extract_min_max_norm_kp(results)\n",
        "\n",
        "                base_video_file_name_wo_extn = os.path.basename(video_name).split(\".mp4\")[0]\n",
        "                dir_name = os.path.join(dest_path, label, base_video_file_name_wo_extn)\n",
        "                npy_path = os.path.join(dir_name, str(frame_num).zfill(4))\n",
        "                #print(\"****\")\n",
        "                #np.save(npy_path, keypoints)\n",
        "                np.save(npy_path, keypoints_min_max_norm)\n",
        "                #print(\"$\")\n",
        "                #print(f\"Mediapipe path : {npy_path}\")\n",
        "\n",
        "            cap.release()\n",
        "            cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "ZzVsUFt812v0"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## collect_and_save_kp_data_for_video_set"
      ],
      "metadata": {
        "id": "9WR_SY_DosRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_and_save_kp_data_for_video_set(source_path, dest_path, sign_language_category = 'asl'):\n",
        "    \"\"\"\n",
        "    This method collects KP data for each video for each label and saves.\n",
        "    \"\"\"\n",
        "    for label in label_list:\n",
        "        collect_and_save_kp_data_for_video(source_path = source_path,\n",
        "                                           dest_path = dest_path,\n",
        "                                           label = label,\n",
        "                                           sign_language_category = sign_language_category,\n",
        "                                           frame_count = MAX_FRAME\n",
        "                                          )"
      ],
      "metadata": {
        "id": "5rVU72zIotet"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mediapipe_data_prep"
      ],
      "metadata": {
        "id": "mQCdc0xAQIYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_data_prep(source_path, sign_language_category = 'asl'):\n",
        "    \"\"\"\n",
        "    This method prepares the media pipe data for LSTM model.\n",
        "    \"\"\"\n",
        "    file_category = get_file_category(source_path = source_path)\n",
        "\n",
        "    video_list, label_val_list = [], []\n",
        "\n",
        "    # Iterating over each label\n",
        "    for label in label_list:\n",
        "\n",
        "        number_of_label_videos = file_count_per_label_dict[sign_language_category][file_category][label][1]\n",
        "        print(f\"Label : {label}, number of label videos: {number_of_label_videos}\")\n",
        "        folder_list = file_count_per_label_dict[sign_language_category][file_category][label][0]\n",
        "\n",
        "        '''\n",
        "        These are folders under which npy files are saved, corresponding to\n",
        "        each video. Under each folder we will have # MAX_FRAME number of npy files.\n",
        "        '''\n",
        "        for folder in folder_list:\n",
        "\n",
        "            video_segment = []\n",
        "            '''\n",
        "            Iterating over each npy files. There will be #MAX_FRAME number\n",
        "            of npy files.\n",
        "            '''\n",
        "            for frame_num in range(1, MAX_FRAME + 1):\n",
        "                dir_name = os.path.join(source_path, label, folder)\n",
        "                #print(f\"dir_name : {dir_name}\")\n",
        "                npy_file_name = os.path.join(dir_name, str(frame_num).zfill(4) + '.npy')\n",
        "                #print(npy_file_name)\n",
        "                frame = np.load(npy_file_name)\n",
        "                video_segment.append(frame)\n",
        "            video_list.append(video_segment)\n",
        "            label_val_list.append(label_map[label])\n",
        "    print(f\"Shape of video_list : {np.array(video_list).shape}\")\n",
        "    print(f\"Shape of label_val_list : {np.array(to_categorical(label_val_list).astype(int)).shape}\")\n",
        "    video_array = np.array(video_list)\n",
        "    label_array = np.array(to_categorical(label_val_list).astype(int))\n",
        "    group_array = np.array(to_categorical(label_val_list).astype(int))\n",
        "    return video_array, label_array, group_array"
      ],
      "metadata": {
        "id": "nQwN-GbmPoRT"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display_model_summary"
      ],
      "metadata": {
        "id": "1EIUmqD__vTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_model_summary(model):\n",
        "    \"\"\"\n",
        "    This method shows model summary.\n",
        "    \"\"\"\n",
        "    model.summary()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    plt.show(block = False)"
      ],
      "metadata": {
        "id": "1m5jyltY_wQT"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display_model_structure"
      ],
      "metadata": {
        "id": "MK49_Eftw1VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_model_structure(model):\n",
        "    \"\"\"\n",
        "    This method shows model structure.\n",
        "    \"\"\"\n",
        "    tf.keras.utils.plot_model(model)"
      ],
      "metadata": {
        "id": "ltIDKDZuw5DS"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display_model_layers"
      ],
      "metadata": {
        "id": "NlMgcqBJw_k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_model_layers(model):\n",
        "    \"\"\"\n",
        "    This method shows model layers.\n",
        "    \"\"\"\n",
        "    for layer in model.layers:\n",
        "        print(layer.get_output_at(0).get_shape().as_list())"
      ],
      "metadata": {
        "id": "ErH9nJIzxCfY"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display_model_weights"
      ],
      "metadata": {
        "id": "fumxVUvfxW_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_model_weights(model):\n",
        "    \"\"\"\n",
        "    This method shows model layers.\n",
        "    \"\"\"\n",
        "    print(model.weights)"
      ],
      "metadata": {
        "id": "cR3hrUNixaH2"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model_evaluation"
      ],
      "metadata": {
        "id": "K2GktrKgBSam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    This method evaluates the test data for a given model.\n",
        "    \"\"\"\n",
        "    test_results = model.evaluate(X_test, y_test)\n",
        "    print('\\nTest Loss : {:.2f}%'.format(test_results[0] * 100))\n",
        "    print('\\nTest Accuracy :  {:.2f}%'.format(test_results[1] * 100))"
      ],
      "metadata": {
        "id": "RLGnfu32BKhN"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display_train_val_loss_and_accuracy"
      ],
      "metadata": {
        "id": "dz1wYkaJzSFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_train_val_loss_and_accuracy(df_hist):\n",
        "    \"\"\"\n",
        "    This method displays training and validation set accuracy and loss.\n",
        "    \"\"\"\n",
        "    # plot loss for train and validation\n",
        "    fig = plt.figure(figsize=(16, 4))\n",
        "    ax = fig.add_subplot(1, 3, 1)\n",
        "    plt.plot(df_hist['loss'], lw=2, color='green')\n",
        "    plt.plot(df_hist['val_loss'], lw=2, color='indianred')\n",
        "    plt.legend(['Train', 'Validation'], fontsize=10)\n",
        "    ax.set_xlabel('Epochs', size=10)\n",
        "    ax.set_title('Loss');\n",
        "\n",
        "    # plot accuracy for train and validation\n",
        "    ax = fig.add_subplot(1, 3, 2)\n",
        "    plt.plot(df_hist['accuracy'], lw=2, color='green')\n",
        "    plt.plot(df_hist['val_accuracy'], lw=2, color='indianred')\n",
        "    plt.legend(['Train', 'Validation'], fontsize=10)\n",
        "    ax.set_xlabel('Epochs', size=10)\n",
        "    ax.set_title('Accuracy');"
      ],
      "metadata": {
        "id": "gHuzFp0vzWY4"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model_prediction"
      ],
      "metadata": {
        "id": "y7_U_Z4yBYhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_prediction(model, X_test):\n",
        "    \"\"\"\n",
        "    This method predicts for a given model.\n",
        "    \"\"\"\n",
        "    # transform logits to probabilities\n",
        "    pred_logits = model.predict(X_test)\n",
        "    probas = tf.sigmoid(self.pred_logits)\n",
        "    probas = probas.numpy().flatten() * 100"
      ],
      "metadata": {
        "id": "zGoaMX4lBZJq"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model_save"
      ],
      "metadata": {
        "id": "y3rKQy8TBBNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_save(model, model_name):\n",
        "    \"\"\"\n",
        "    This method saves the model in a h5 file.\n",
        "    \"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    model.save(model_name + '.h5')"
      ],
      "metadata": {
        "id": "DczF9Pl5A8Iv"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model_fit"
      ],
      "metadata": {
        "id": "pdh3FaTkx6P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fit(model, X_train, y_train, epochs, batch_size, X_val, y_val):\n",
        "    \"\"\"\n",
        "    This method fits the model.\n",
        "    \"\"\"\n",
        "    history = model.fit(X_train,\n",
        "                        y_train,\n",
        "                        epochs = epochs,\n",
        "                        batch_size = batch_size,\n",
        "                        #steps_per_epoch = 4,\n",
        "                        validation_data = (X_val, y_val)\n",
        "                       )\n",
        "    hist = history.history\n",
        "    df_hist = pd.DataFrame(hist)\n",
        "    return df_hist"
      ],
      "metadata": {
        "id": "18QKDuvUx8FF"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_folder_in_google_drive"
      ],
      "metadata": {
        "id": "-UwZak2Ws-qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_folder_in_google_drive(src_dr, tgt_dir):\n",
        "    \"\"\"\n",
        "    This method saves the directory in google drive.\n",
        "    \"\"\"\n",
        "    !cp -r {src_dir} {tgt_dir}"
      ],
      "metadata": {
        "id": "J4KoWQNztEQg"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_video_seq"
      ],
      "metadata": {
        "id": "q2v0TNfHwaPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_seq(df):\n",
        "    \"\"\"\n",
        "    This method creates video sequence and corresponding label.\n",
        "    \"\"\"\n",
        "    #global df_file_list\n",
        "    video_sequence = []\n",
        "    #video_sequence_label = []\n",
        "    df_len = df.shape[0]\n",
        "    shuffle = np.random.permutation(np.arange(df.shape[0]).tolist())\n",
        "    df = df.iloc[shuffle]\n",
        "\n",
        "    #df_file_list = df_file_list.sample(frac = 1)\n",
        "    for video_data in df[['label', 'qualified_file_name']].values:\n",
        "        video_file = video_data[1]\n",
        "        video_label = video_data[0]\n",
        "        video_sequence.append(skvideo.utils.rgb2gray(skvideo.io.vread(video_file)))\n",
        "        #video_sequence_label.append(label_map[video_label])\n",
        "\n",
        "    X = np.array(video_sequence)\n",
        "    X = X.astype(np.float)\n",
        "    #y = np.array(to_categorical(video_sequence_label).astype(int))\n",
        "    #print(X.shape, y.shape)\n",
        "    return X"
      ],
      "metadata": {
        "id": "sXuW9s_Wv8Kd"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_images_from_video"
      ],
      "metadata": {
        "id": "eHPpQmUnpnpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_images_from_video(qualified_video_file_name, tgt_path):\n",
        "    \"\"\"\n",
        "    This method reads the input video and returns the set of images/frames\n",
        "    and also saves them in the specified path.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(qualified_video_file_name)\n",
        "    image_file_list = []\n",
        "    for frame in range(MAX_FRAME):\n",
        "        ret, image = cap.read()\n",
        "        if image is None:\n",
        "            break\n",
        "        if check_image(image) == True:\n",
        "            continue\n",
        "        image = 255 * np.ones((IMAGE_SIZE, IMAGE_SIZE), dtype = np.uint8)\n",
        "        base_video_file_name = os.path.basename(qualified_video_file_name)\n",
        "        file_name = base_video_file_name.split(\".mp4\")[0] + \"_f\" + str(frame).zfill(4) + \".jpeg\"\n",
        "        img_file_name = os.path.join(tgt_path, file_name)\n",
        "        #print(img_file_name)\n",
        "        plt.imsave(img_file_name, image, cmap='gray')\n",
        "        image_file_list.append(image)\n",
        "    return np.array(image_file_list) / 255"
      ],
      "metadata": {
        "id": "cOGKOL4lpsMK"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_images_from_video_set"
      ],
      "metadata": {
        "id": "aKsFYKxbuB3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_images_from_video_set(src_path, dir_path):\n",
        "    \"\"\"\n",
        "    This method reads the input video and returns the set of images/frames\n",
        "    and also saves them in the specified path.\n",
        "    \"\"\"\n",
        "    video_data = []\n",
        "    for label in label_list:\n",
        "        video_dir = os.path.join(src_path, label)\n",
        "        for video_file in os.listdir(video_dir):\n",
        "            qualified_video_file_name = os.path.join(video_dir, video_file)\n",
        "            #print(f\"Processing label {label} : {qualified_video_file_name}\")\n",
        "            tgt_path = os.path.join(dir_path, label)\n",
        "            video_frames = get_images_from_video(qualified_video_file_name = qualified_video_file_name,\n",
        "                                                 tgt_path = tgt_path\n",
        "                                                )\n",
        "            video_data.append((video_frames, label_map[label]))\n",
        "    return video_data"
      ],
      "metadata": {
        "id": "FzkgFKb4uEtC"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## move_synthetic_gen_file_under_augmented_path"
      ],
      "metadata": {
        "id": "I22MgOb_fdR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def move_synthetic_gen_file_under_augmented_path(src_path, tgt_path):\n",
        "    \"\"\"\n",
        "    This method moves synthetically augmented file under resoective augmented path.\n",
        "    \"\"\"\n",
        "    for label in label_list:\n",
        "        dir_path = os.path.join(src_path, label)\n",
        "        dest_path = os.path.join(tgt_path, label)\n",
        "        file_list = [file for file in os.listdir(dir_path) if \"_syn_v\" in file]\n",
        "        for file in file_list:\n",
        "             #move_list.append(os.path.join(video_path, file))\n",
        "             move_file = os.path.join(dir_path, file)\n",
        "             !mv {move_file} {dest_path}"
      ],
      "metadata": {
        "id": "YY5v6voifcHr"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## stratify_and_get_train_val_test_data"
      ],
      "metadata": {
        "id": "4g-mHlsoDk1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stratify_and_get_train_val_test_data():\n",
        "    \"\"\"\n",
        "    This method stratifies data across labels and returns train, val and test data.\n",
        "    \"\"\"\n",
        "    global PER_LABEL_IMAGE_COUNT\n",
        "    global PER_LABEL_AUGMENTED_IMAGE_COUNT\n",
        "    global df_file_list\n",
        "    global splits\n",
        "\n",
        "    df_orig = pd.DataFrame()\n",
        "    df_aug = pd.DataFrame()\n",
        "    df_train = pd.DataFrame()\n",
        "    df_val = pd.DataFrame()\n",
        "    df_test = pd.DataFrame()\n",
        "\n",
        "    for label in label_list:\n",
        "\n",
        "        # Get category wise transformed and augmented data\n",
        "        temp_df_orig = df_file_list[(df_file_list.label == label) & (df_file_list.file_category == 'transformed_images')]\n",
        "        temp_df_aug = df_file_list[(df_file_list.label == label) & (df_file_list.file_category == 'transformed_augmented_images')]\n",
        "\n",
        "        # Shuffle the data\n",
        "        shuffle_orig = np.random.permutation(np.arange(len(temp_df_orig)))\n",
        "        shuffle_aug = np.random.permutation(np.arange(len(temp_df_aug)))\n",
        "        temp_df_orig = temp_df_orig.iloc[shuffle_orig]\n",
        "        temp_df_aug = temp_df_aug.iloc[shuffle_aug]\n",
        "\n",
        "        # Sample data per PER_LABEL_IMAGE_COUNT and PER_LABEL_AUGMENTED_IMAGE_COUNT\n",
        "        temp_df_orig_sample = temp_df_orig.sample(n = PER_LABEL_IMAGE_COUNT, replace = False)\n",
        "        temp_df_aug_sample = temp_df_aug.sample(n = PER_LABEL_AUGMENTED_IMAGE_COUNT, replace = False)\n",
        "\n",
        "        # Split the transformed data in init, val and test\n",
        "        temp_df_init = pd.DataFrame(temp_df_orig_sample.iloc[:splits[0]])\n",
        "        temp_df_val = pd.DataFrame(temp_df_orig_sample.iloc[splits[0] : splits[0] + splits[1]])\n",
        "        temp_df_test = pd.DataFrame(temp_df_orig_sample.iloc[splits[0] + splits[1] : ])\n",
        "\n",
        "        # Append augmented data in init to form train\n",
        "        temp_df_train = pd.concat([temp_df_init, temp_df_aug_sample], axis=0)\n",
        "\n",
        "        df_train = pd.concat([df_train, temp_df_train], axis=0)\n",
        "        df_val = pd.concat([df_val, temp_df_val], axis=0)\n",
        "        df_test = pd.concat([df_test, temp_df_test], axis=0)\n",
        "\n",
        "        print(f\"Label: {label}\")\n",
        "        print(f\"Length of temp_df_init: {len(temp_df_init)}\")\n",
        "        print(f\"Length of temp_df_aug: {len(temp_df_aug)}\")\n",
        "        print(f\"Length of temp_df_train: {len(temp_df_train)}\")\n",
        "        print(f\"Length of temp_df_val: {len(temp_df_val)}\")\n",
        "        print(f\"Length of temp_df_test: {len(temp_df_test)}\")\n",
        "\n",
        "    return df_train, df_val, df_test"
      ],
      "metadata": {
        "id": "1PRe_qifDg-n"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_val_test_file_move"
      ],
      "metadata": {
        "id": "SWzs-ta6b3H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_test_file_move(df, dest_path):\n",
        "    \"\"\"\n",
        "    Move files from processed folder to train/val/test folders.\n",
        "    \"\"\"\n",
        "    for label in label_list:\n",
        "        tgt_path = os.path.join(dest_path, label)\n",
        "        file_list = df[df.label == label]['qualified_file_name'].values.tolist()\n",
        "        print(f\"{len(file_list)} files to be moved to {tgt_path}\")\n",
        "        process_copy_files(file_name_list = file_list,\n",
        "                           dest_path = tgt_path\n",
        "                          )\n",
        "        cmd_string = f'ls -ltr {tgt_path}/|wc -l'\n",
        "        os.system(cmd_string)"
      ],
      "metadata": {
        "id": "iB0v15zab19X"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_X_y_data"
      ],
      "metadata": {
        "id": "6eQalR95t4Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y_data():\n",
        "\n",
        "    train_X_list = []\n",
        "    train_y_list = []\n",
        "    val_X_list = []\n",
        "    val_y_list = []\n",
        "    test_X_list = []\n",
        "    test_y_list = []\n",
        "    for label in label_list:\n",
        "\n",
        "        train_path = os.path.join(TRAIN_DATA_DIR, label)\n",
        "        val_path = os.path.join(VAL_DATA_DIR, label)\n",
        "        test_path = os.path.join(TEST_DATA_DIR, label)\n",
        "\n",
        "        temp_X_train = load_images_from_folder(folder = train_path, file_extension = 'jpeg')\n",
        "        temp_X_val = load_images_from_folder(folder = val_path, file_extension = 'jpeg')\n",
        "        temp_X_test = load_images_from_folder(folder = test_path, file_extension = 'jpeg')\n",
        "\n",
        "        train_X_list.append(temp_X_train)\n",
        "        val_X_list.append(temp_X_val)\n",
        "        test_X_list.append(temp_X_test)\n",
        "\n",
        "        temp_y_train = np.empty(len(temp_X_train))\n",
        "        temp_y_train.fill(label_map[label])\n",
        "        train_y_list.append(temp_y_train)\n",
        "\n",
        "        temp_y_val = np.empty(len(temp_X_val))\n",
        "        temp_y_val.fill(label_map[label])\n",
        "        val_y_list.append(temp_y_val)\n",
        "\n",
        "        temp_y_test = np.empty(len(temp_X_test))\n",
        "        temp_y_test.fill(label_map[label])\n",
        "        test_y_list.append(temp_y_test)\n",
        "\n",
        "    X_train = np.concatenate(train_X_list)\n",
        "    X_val = np.concatenate(val_X_list)\n",
        "    X_test = np.concatenate(test_X_list)\n",
        "\n",
        "    y_train = np.concatenate(np.array(to_categorical(train_y_list).astype(int)))\n",
        "    y_val = np.concatenate(np.array(to_categorical(val_y_list).astype(int)))\n",
        "    y_test = np.concatenate(np.array(to_categorical(test_y_list).astype(int)))\n",
        "\n",
        "    shuffle_train = np.random.permutation(np.arange(len(X_train)))\n",
        "    X_train, y_train = X_train[shuffle_train], y_train[shuffle_train]\n",
        "\n",
        "    shuffle_val = np.random.permutation(np.arange(len(X_val)))\n",
        "    X_val, y_val = X_val[shuffle_val], y_val[shuffle_val]\n",
        "\n",
        "    shuffle_test = np.random.permutation(np.arange(len(X_test)))\n",
        "    X_test, y_test = X_test[shuffle_test], y_test[shuffle_test]\n",
        "\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "    print(\"X_val shape:\", X_val.shape)\n",
        "    print(\"y_val shape:\", y_val.shape)\n",
        "\n",
        "    print(\"X_test shape:\", X_test.shape)\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "metadata": {
        "id": "lCCK9jsst2AY"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_cnn_model_v1"
      ],
      "metadata": {
        "id": "1MzjYH6pslK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_model_v1(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "W4ExzApTsoqV"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert_mp4_to_avi"
      ],
      "metadata": {
        "id": "oY4o350yHGir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_mp4_to_avi(src_path, tgt_path):\n",
        "    for file in os.listdir(src_path):\n",
        "        src_qualified_file_name = os.path.join(src_path, file)\n",
        "        tgt_file = file.split(\".mp4\")[0] + \".avi\"\n",
        "        tgt_qualified_file_name = os.path.join(tgt_path, tgt_file)\n",
        "        ff = ffmpy.FFmpeg(inputs  = {src_qualified_file_name: None},\n",
        "                          outputs = {tgt_qualified_file_name: None}\n",
        "                         )\n",
        "        ff.run()"
      ],
      "metadata": {
        "id": "CDi9uJKQHFej"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert_mp4_to_avi_set"
      ],
      "metadata": {
        "id": "aFtFULheHOsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_mp4_to_avi_set(label_list, src_path, tgt_path):\n",
        "    for label in label_list:\n",
        "        src_qualified_path = os.path.join(src_path)\n",
        "        tgt_qualified_path = os.path.join(tgt_path)\n",
        "        convert_mp4_to_avi(src_path = src_qualified_path, tgt_path = tgt_qualified_path)"
      ],
      "metadata": {
        "id": "qVqlOLABHM1j"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "AmSaZT8WCJxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Seed"
      ],
      "metadata": {
        "id": "CPud-30jVhNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Setting seed values\")\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "Snm4cv6q-7Dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09c102e-0a5b-4152-d872-8f4346d1d1dc"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting seed values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Directory Setup"
      ],
      "metadata": {
        "id": "rwn6b-ve2GDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Removing existing directory structures\")\n",
        "!rm -rf asl_dataset/ isl_dataset/\n",
        "remove_temp_dir()"
      ],
      "metadata": {
        "id": "HGqol-H1MpE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4501cb0f-2519-4425-dee5-8f9d8ff44d1c"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing directory structures\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Removing existing directory structures\")\n",
        "create_dir(os.path.join(os.getcwd(), \"temp\"))\n",
        "create_dir(os.path.join(os.getcwd(), \"temp_aug\"))\n",
        "create_dir_structure(sign_language_category = \"asl\")"
      ],
      "metadata": {
        "id": "0HRzg1VD2LTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06a0b52-7a10-4a46-b61b-094e7929bc0c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing directory structures\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprare Stage Data Set for the Label Set"
      ],
      "metadata": {
        "id": "5AgKMjR5SCfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Collecting input data\")\n",
        "set_input_output_map_dict()\n",
        "prepare_input_dataset_for_label_set(sign_language_category = \"asl\",\n",
        "                                    stage_dir_path = STAGE_DATA_DIR,\n",
        "                                    label_set = label_list\n",
        "                                   )"
      ],
      "metadata": {
        "id": "3D8w6MRS2ZwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65eaecb-fcea-472b-fa69-305a4d721c09"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting input data\n",
            "There are 10 files for label love\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://signstock.blob.core.windows.net/signschool/videos/SignSchool%20Love.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://signstock.blob.core.windows.net/signschool/videos/SignSchool%20Love.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://media.asldeafined.com/vocabulary/1468712876.4162.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://media.asldeafined.com/vocabulary/1468712876.4162.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://www.signingsavvy.com/signs/mp4/22/22883.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://www.signingsavvy.com/signs/mp4/22/22883.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://www.signingsavvy.com/signs/mp4/23/23180.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://www.signingsavvy.com/signs/mp4/23/23180.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://s3-us-west-1.amazonaws.com/files.startasl.com/asldictionary/love.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://s3-us-west-1.amazonaws.com/files.startasl.com/asldictionary/love.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://signstock.blob.core.windows.net/signschool/videos/SignSchool%20Love.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://signstock.blob.core.windows.net/signschool/videos/SignSchool%20Love.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://media.asldeafined.com/vocabulary/1468712876.4162.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://media.asldeafined.com/vocabulary/1468712876.4162.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://www.signingsavvy.com/signs/mp4/22/22883.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://www.signingsavvy.com/signs/mp4/22/22883.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://www.signingsavvy.com/signs/mp4/23/23180.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://www.signingsavvy.com/signs/mp4/23/23180.mp4?downloadformat=mp4 True 200\n",
            "\n",
            "\n",
            "--------------------------------\n",
            "Processing https://s3-us-west-1.amazonaws.com/files.startasl.com/asldictionary/love.mp4\n",
            "--------------------------------\n",
            "Input file is an embedded video from a site other than youtube.\n",
            "https://s3-us-west-1.amazonaws.com/files.startasl.com/asldictionary/love.mp4?downloadformat=mp4 True 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = label_list[0]"
      ],
      "metadata": {
        "id": "7RzRfYmzEcuq"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/asl_dataset/stage/{label}/video/temp_*"
      ],
      "metadata": {
        "id": "s8T26JXIGf0L"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltr /content/asl_dataset/stage/{label}/video/*.mp4|wc -l"
      ],
      "metadata": {
        "id": "aS3xjwJD7B5F",
        "outputId": "1d0f41e2-fdff-44bc-ae86-4f3ba3f2b26d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh /content/asl_dataset/stage/{label}/video/*.mp4"
      ],
      "metadata": {
        "id": "B_u3yEcYCP9T",
        "outputId": "835b7f2c-d632-4dbb-f572-840bcd128d46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root  55K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root  55K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root  69K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f76fa426_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root  69K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 227K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 227K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f4a57590_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 606K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f81f4610_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 606K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f653c964_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 842K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f71b874c_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 842K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f5422e30_91a1_11ee_82ac_0242ac1c000c.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/asl_dataset/stage/{label}/video/ -type f -name *mp4 -size -1k -delete"
      ],
      "metadata": {
        "id": "lr6Iy2fsF6yQ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh /content/asl_dataset/stage/{label}/video/*.mp4"
      ],
      "metadata": {
        "id": "dfyp1lztGace",
        "outputId": "2b8b292c-b7c6-41a3-fc1f-0e49b5d4b634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root  55K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root  55K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root  69K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f76fa426_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root  69K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 227K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 227K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f4a57590_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 606K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f81f4610_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 606K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f653c964_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 842K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f71b874c_91a1_11ee_82ac_0242ac1c000c.mp4\n",
            "-rw-r--r-- 1 root root 842K Dec  3 06:05 /content/asl_dataset/stage/love/video/love_f5422e30_91a1_11ee_82ac_0242ac1c000c.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir_path = os.path.join(STAGE_DATA_DIR, label, 'video')\n",
        "tgt_dir_path = os.path.join(STAGE_DATA_DIR, label, 'video_aug')\n",
        "video_augmentation(src_dir_path = src_dir_path,\n",
        "                   tgt_dir_path = tgt_dir_path\n",
        "                  )"
      ],
      "metadata": {
        "id": "Jq2nVto0CwHi",
        "outputId": "3cf0896a-941d-46a8-bd48-51ab32fd813f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Augmentation function: sharpen\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_brighntness\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "**************************************************\n",
            "Augmentation function: adj_gamma\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "**************************************************\n",
            "Augmentation function: flip\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_flip.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "**************************************************\n",
            "Augmentation function: rotate_90_or_180\n",
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "**************************************************\n",
            "Augmentation function: make_fast\n",
            "**************************************************\n",
            "Augmentation function: make_slow\n",
            "**************************************************\n",
            "Augmentation function: rotate_by_angle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4.\n",
            "Moviepy - Writing video /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh /content/asl_dataset/stage/{label}/video_aug/*.mp4"
      ],
      "metadata": {
        "id": "jXD064JRH6S1",
        "outputId": "3d625638-fa0f-4b42-b933-0a32ca4ff590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 6.2K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root 6.2K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root 6.4K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root 6.4K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root 6.4K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root 6.4K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root 6.4K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root 6.4K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root 7.8K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root 7.8K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root 9.3K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root 9.3K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root 9.3K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root 9.3K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root 9.3K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root 9.3K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root  12K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root  12K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root  12K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root  12K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root  12K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root  12K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root  13K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root  18K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root  19K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root  19K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root  19K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root  19K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root  19K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root  19K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root  20K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root  20K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root  20K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root  23K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root  23K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root  24K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root  24K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root  28K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root  28K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root  30K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root  31K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root  31K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root  31K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root  32K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root  46K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root  47K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root  49K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root  50K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root  51K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root  51K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root  52K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root  54K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root  60K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root  60K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root  60K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root  60K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_sharpen.mp4\n",
            "-rw-r--r-- 1 root root  60K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_adj_gamma.mp4\n",
            "-rw-r--r-- 1 root root  60K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_adj_brighntness.mp4\n",
            "-rw-r--r-- 1 root root  75K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root  75K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root  79K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5a281e0_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root  88K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f76fa426_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root  92K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root 115K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root 116K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root 118K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root 118K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root 126K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_rotate_90_or_180.mp4\n",
            "-rw-r--r-- 1 root root 128K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root 135K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root 135K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_fast.mp4\n",
            "-rw-r--r-- 1 root root 144K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_flip.mp4\n",
            "-rw-r--r-- 1 root root 223K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root 233K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root 233K Dec  3 06:05 /content/asl_dataset/stage/love/video_aug/love_f4a57590_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root 279K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_rotate_by_angle.mp4\n",
            "-rw-r--r-- 1 root root 456K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f71b874c_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root 456K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f5422e30_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root 533K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f81f4610_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n",
            "-rw-r--r-- 1 root root 533K Dec  3 06:06 /content/asl_dataset/stage/love/video_aug/love_f653c964_91a1_11ee_82ac_0242ac1c000c_slow.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/asl_dataset/stage/{label}/video_aug/ -type f -name *mp4 -size -1k -delete"
      ],
      "metadata": {
        "id": "J6h0S6djImok"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh /content/asl_dataset/stage/{label}/video_aug/*.mp4|wc -l"
      ],
      "metadata": {
        "id": "UKXMSMxHI_zP",
        "outputId": "2f694e61-2f40-4462-e0ec-68f5d1e4c1b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {os.environ['GDRIVE_CONFIG_DIR']}/{label}_mp4_videos/*\n",
        "!rm -rf {os.environ['GDRIVE_CONFIG_DIR']}/{label}_avi_videos/*"
      ],
      "metadata": {
        "id": "xWKAfG3l6IOQ"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/asl_dataset/stage/{label}/video/ {os.environ['GDRIVE_CONFIG_DIR']}/{label}_mp4_videos\n",
        "!cp -r /content/asl_dataset/stage/{label}/video_aug/ {os.environ['GDRIVE_CONFIG_DIR']}/{label}_mp4_videos"
      ],
      "metadata": {
        "id": "gq0SZ0d77Oyc"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh {os.environ['GDRIVE_CONFIG_DIR']}/{label}_mp4_videos/video/*.mp4|wc -l"
      ],
      "metadata": {
        "id": "wYiu9Lsn7isc",
        "outputId": "b55a8bbc-bd51-4745-c05d-df3c9a02223a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh {os.environ['GDRIVE_CONFIG_DIR']}/{label}_mp4_videos/video_aug/*.mp4|wc -l"
      ],
      "metadata": {
        "id": "mXzpUYigI1Hg",
        "outputId": "474645d4-5e3f-4ef4-b8cf-841333dcfe00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_mp4_to_avi_set(label_list = [label],\n",
        "                       src_path = os.environ['GDRIVE_CONFIG_DIR'] + f'/{label}_mp4_videos/video',\n",
        "                       tgt_path = os.environ['GDRIVE_CONFIG_DIR'] + f'/{label}_avi_videos'\n",
        "                      )\n",
        "convert_mp4_to_avi_set(label_list = [label],\n",
        "                       src_path = os.environ['GDRIVE_CONFIG_DIR'] + f'/{label}_mp4_videos/video_aug',\n",
        "                       tgt_path = os.environ['GDRIVE_CONFIG_DIR'] + f'/{label}_avi_videos'\n",
        "                      )"
      ],
      "metadata": {
        "id": "s6RDW2ZLzqTt"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh {os.environ['GDRIVE_CONFIG_DIR']}/{label}_mp4_videos/video/*.mp4|wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7HZcep9ZRwS",
        "outputId": "31dffcc6-ad98-43a8-c39d-fcda525e1a1f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh {os.environ['GDRIVE_CONFIG_DIR']}/{label}_mp4_videos/video_aug/*.mp4|wc -l"
      ],
      "metadata": {
        "id": "RQK1BgEaJOW5",
        "outputId": "13e88a42-3c6e-4087-f048-efdc5cb2c2b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrSh {os.environ['GDRIVE_CONFIG_DIR']}/{label}_avi_videos/*.avi|wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm1gHXIeZbfM",
        "outputId": "075225d4-6f27-47ba-bdb1-cb83ae01eaeb"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_output_map_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFQWKaRHaNhJ",
        "outputId": "1a88b6f6-2fac-4d0d-99b5-c4d0f7f5063c"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'love': [('https://signstock.blob.core.windows.net/signschool/videos/SignSchool%20Love.mp4',\n",
              "   'love_f4a57590_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://media.asldeafined.com/vocabulary/1468712876.4162.mp4',\n",
              "   'love_f5422e30_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://www.signingsavvy.com/signs/mp4/22/22883.mp4',\n",
              "   'love_f5a281e0_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://www.signingsavvy.com/signs/mp4/23/23180.mp4',\n",
              "   'love_f5ea1b04_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://s3-us-west-1.amazonaws.com/files.startasl.com/asldictionary/love.mp4',\n",
              "   'love_f653c964_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://signstock.blob.core.windows.net/signschool/videos/SignSchool%20Love.mp4',\n",
              "   'love_f6c8ebfe_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://media.asldeafined.com/vocabulary/1468712876.4162.mp4',\n",
              "   'love_f71b874c_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://www.signingsavvy.com/signs/mp4/22/22883.mp4',\n",
              "   'love_f76fa426_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://www.signingsavvy.com/signs/mp4/23/23180.mp4',\n",
              "   'love_f7bbb6fe_91a1_11ee_82ac_0242ac1c000c.mp4'),\n",
              "  ('https://s3-us-west-1.amazonaws.com/files.startasl.com/asldictionary/love.mp4',\n",
              "   'love_f81f4610_91a1_11ee_82ac_0242ac1c000c.mp4')]}"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    }
  ]
}